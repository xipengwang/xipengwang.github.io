{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "according-victor",
   "metadata": {},
   "source": [
    "# Machine Learning Notes - Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-choice",
   "metadata": {},
   "source": [
    "## A List of Related Posts\n",
    "1. [Pytorch(this)]({% post_url 2021-05-04-machine-learning-pytorch %})\n",
    "2. [Loss function]({% post_url 2021-05-07-machine-learning-loss %})\n",
    "3. [Backpropagation]({% post_url 2021-05-07-machine-learning-backpropagation %})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-colorado",
   "metadata": {},
   "source": [
    "## Pytorch Introduction\n",
    "[PyTorch](https://pytorch.org/) is an open source machine learning framework. \n",
    "You can find more information about PyTorch by following one of the [oficial tutorials](https://pytorch.org/tutorials/) or by [reading the documentation](https://pytorch.org/docs/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-dealing",
   "metadata": {},
   "source": [
    "## Import Pythorch\n",
    "<code>\n",
    "    torch.cude.is_available()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "third-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "Is cuda available? False\n"
     ]
    }
   ],
   "source": [
    "# Import pytorch and check its version\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "print(torch.__version__)\n",
    "print(f'Is cuda available? {torch.cuda.is_available()}')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-albuquerque",
   "metadata": {},
   "source": [
    "## Pytorch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-physics",
   "metadata": {},
   "source": [
    "### Tensor Initialization\n",
    "<code>\n",
    "    torch.tensor(),\n",
    "    torch.from_numpy(),\n",
    "    torch.zerors_like(),\n",
    "    torch.ones_like(),\n",
    "    torch.rand(),\n",
    "    torch.ones(),\n",
    "    torch.zeros(),\n",
    "    torch.eye(),\n",
    "    torch.full(),\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "wrong-information",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Tensor: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Numpy Tensor: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0, 0],\n",
      "        [0, 0]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.3355, 0.0668],\n",
      "        [0.2169, 0.7994]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0993, 0.1139, 0.0951],\n",
      "        [0.6038, 0.0597, 0.6543]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Full twos Tensor: \n",
      " tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor Initialization\n",
    "\n",
    "# Directly from data\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Direct Tensor: \\n {x_np} \\n\")\n",
    "\n",
    "# From a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Numpy Tensor: \\n {x_np} \\n\")\n",
    "\n",
    "# From another tensor:\n",
    "x_zeros = torch.zeros_like(x_data)\n",
    "print(f\"Zeros Tensor: \\n {x_zeros} \\n\")\n",
    "\n",
    "x_ones = torch.ones_like(x_data) \n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) \n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "full_tensor = torch.full(shape, 2)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\\n\")\n",
    "print(f\"Full twos Tensor: \\n {full_tensor}\\n\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-performance",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "<code>\n",
    "    tensor.dim(),\n",
    "    tensor.shape,\n",
    "    tensor.dtype,\n",
    "    tensor.device,\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "stylish-trade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of tensor: 2\n",
      "\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "\n",
      "Datatype of tensor: torch.float32\n",
      "\n",
      "Device tensor is stored on: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor Atrributes\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Dimension of tensor: {tensor.dim()}\\n\")\n",
    "print(f\"Shape of tensor: {tensor.shape}\\n\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\\n\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\\n\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-conducting",
   "metadata": {},
   "source": [
    "### Tensor Indexing\n",
    "<code>\n",
    "    tensor[start:stop:step]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "featured-cassette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "shape:  torch.Size([3, 4])\n",
      "\n",
      "Single row:\n",
      "torch.Size([4]) tensor([5, 6, 7, 8])\n",
      "shape:  torch.Size([4]) torch.Size([4])\n",
      "\n",
      "Single column:\n",
      "tensor([ 2,  6, 10])\n",
      "shape:  torch.Size([3])\n",
      "\n",
      "First two rows, last two columns:\n",
      "tensor([[2, 3, 4],\n",
      "        [6, 7, 8]])\n",
      "shape:  torch.Size([2, 3])\n",
      "\n",
      "Every other row, middle columns:\n",
      "tensor([[ 2,  3],\n",
      "        [10, 11]])\n",
      "shape:  torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Tensor slicing\n",
    "x = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print('Original tensor:')\n",
    "print(x)\n",
    "print('shape: ', x.shape)\n",
    "\n",
    "# Get row 1, and all columns. \n",
    "print('\\nSingle row:')\n",
    "print(x[1, :].shape, x[1, :])\n",
    "print('shape: ', x[1, :].shape, x[1].shape)  \n",
    "\n",
    "print('\\nSingle column:')\n",
    "print(x[:, 1])\n",
    "print('shape: ', x[:, 1].shape)\n",
    "\n",
    "# Get the first two rows and the last three columns\n",
    "print('\\nFirst two rows, last two columns:')\n",
    "print(x[:2, -3:])\n",
    "print('shape: ', x[:2, -3:].shape)\n",
    "\n",
    "# Get every other row, and columns at index 1 and 2\n",
    "print('\\nEvery other row, middle columns:')\n",
    "print(x[::2, 1:3])\n",
    "print('shape: ', x[::2, 1:3].shape)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-reunion",
   "metadata": {},
   "source": [
    "More generally, given index arrays `idx0` and `idx1` with `N` elements each, `a[idx0, idx1]` is equivalent to:\n",
    "\n",
    "```\n",
    "torch.tensor([\n",
    "  a[idx0[0], idx1[0]],\n",
    "  a[idx0[1], idx1[1]],\n",
    "  ...,\n",
    "  a[idx0[N - 1], idx1[N - 1]]\n",
    "])\n",
    "```\n",
    "\n",
    "(A similar pattern extends to tensors with more than two dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "turkish-christmas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "shape:  torch.Size([3, 4])\n",
      "\n",
      "Reordered columns:\n",
      "tensor([[ 4,  3,  2,  1],\n",
      "        [ 8,  7,  6,  5],\n",
      "        [12, 11, 10,  9]])\n",
      "\n",
      "Reordered rows/columns:\n",
      "tensor([4, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "# Integer Index\n",
    "\n",
    "x = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print('Original tensor:')\n",
    "print(x)\n",
    "print('shape: ', x.shape)\n",
    "\n",
    "idx = torch.tensor([3, 2, 1, 0])  # Index arrays can be int64 torch tensors\n",
    "print('\\nReordered columns:')\n",
    "print(x[:, idx])\n",
    "\n",
    "\n",
    "a = torch.tensor([0, 1, 0])  # Index arrays can be int64 torch tensors\n",
    "b = torch.tensor([3, 2, 1])  # Index arrays can be int64 torch tensors\n",
    "print('\\nReordered rows/columns:')\n",
    "print(x[a, b])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "continued-anger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "Mask tensor:\n",
      "tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n",
      "\n",
      "Selecting elements with the mask:\n",
      "tensor([4, 5, 6])\n",
      "\n",
      "After modifying with a mask:\n",
      "tensor([[0, 0],\n",
      "        [0, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Boolen indexing\n",
    "x = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
    "print('Original tensor:')\n",
    "print(x)\n",
    "\n",
    "mask = (x > 3)\n",
    "print('\\nMask tensor:')\n",
    "print(mask)\n",
    "\n",
    "# We can use the mask to construct a rank-1 tensor containing the elements of a\n",
    "# that are selected by the mask\n",
    "print('\\nSelecting elements with the mask:')\n",
    "print(x[mask])\n",
    "\n",
    "# We can also use boolean masks to modify tensors; for example this sets all\n",
    "# elements <= 3 to zero:\n",
    "x[x <= 3] = 0\n",
    "print('\\nAfter modifying with a mask:')\n",
    "print(x)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-myanmar",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "<code>\n",
    "    tensor.cat(), torch.sum(), torch.mean(), torch.max(), torch.min(), torch.dot(),\n",
    "    torch.mm(), torch.mv(), torch.addmm(), torch,addmv(),torch.bmm(), torch,baddmm(),\n",
    "    torch.matmul(), torch.torch.broadcast_tensors()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-tractor",
   "metadata": {},
   "source": [
    "#### Tensor Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "indoor-romania",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 2., 1.],\n",
      "        [1., 0., 1., 3.]])\n",
      "\n",
      "horizontal cat:\n",
      " tensor([[1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 2., 1., 1., 0., 2., 1.],\n",
      "        [1., 0., 1., 3., 1., 0., 1., 3.]])\n",
      "\n",
      "vetical cat:\n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 2., 1.],\n",
      "        [1., 0., 1., 3.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 2., 1.],\n",
      "        [1., 0., 1., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor Operations\n",
    "\n",
    "# Standard numpy-like indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "tensor[2,2] = 2\n",
    "tensor[3,3] = 3\n",
    "print(f'{tensor}\\n')\n",
    "\n",
    "# When use `torch.cat` and specify `dim=x`, then the dimension `x` will increase. \n",
    "t_h = torch.cat([tensor, tensor], dim=1)\n",
    "print(f'horizontal cat:\\n {t_h}\\n')\n",
    "\n",
    "t_v = torch.cat([tensor, tensor], dim=0)\n",
    "print(f'vetical cat:\\n {t_v}\\n')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-benjamin",
   "metadata": {},
   "source": [
    "#### Elementwise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "editorial-heaven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementwise sum:\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "tensor([[ 6.,  8., 10., 12.]])\n",
      "\n",
      "Elementwise difference:\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "tensor([[-4., -4., -4., -4.]])\n",
      "\n",
      "Elementwise product:\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "tensor([[ 5., 12., 21., 32.]])\n",
      "\n",
      "Elementwise division\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
      "\n",
      "Elementwise power\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
      "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n"
     ]
    }
   ],
   "source": [
    "# Elementwise operations\n",
    "\n",
    "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
    "y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Elementwise sum; all give the same result\n",
    "print('Elementwise sum:')\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "\n",
    "# Elementwise difference\n",
    "print('\\nElementwise difference:')\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x.sub(y))\n",
    "\n",
    "# Elementwise product\n",
    "print('\\nElementwise product:')\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "print(x.mul(y))\n",
    "\n",
    "# Elementwise division\n",
    "print('\\nElementwise division')\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "print(x.div(y))\n",
    "\n",
    "# Elementwise power\n",
    "print('\\nElementwise power')\n",
    "print(x ** y)\n",
    "print(torch.pow(x, y))\n",
    "print(x.pow(y))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-imperial",
   "metadata": {},
   "source": [
    "#### Reduction Operations\n",
    "\n",
    "Reduction operations *reduce* the rank of tensors: the dimension over which you perform the reduction will be removed from the shape of the output. If you pass `keepdim=True` to a reduction operation, the specified dimension will not be removed; the output tensor will instead have a shape of 1 in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "organic-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Sum over entire tensor:\n",
      "tensor(21.)\n",
      "\n",
      "Sum of each row:\n",
      "torch.Size([3])\n",
      "\n",
      "Sum of each column:\n",
      "tensor([ 6., 15.])\n"
     ]
    }
   ],
   "source": [
    "# Reduction\n",
    "\n",
    "# When use reduction and specify `dim=x`, then the dimension `x` will be removed. \n",
    "\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "print('Original tensor:')\n",
    "print(x)\n",
    "\n",
    "print('\\nSum over entire tensor:')\n",
    "print(torch.sum(x))\n",
    "\n",
    "# We can sum over each row:\n",
    "print('\\nSum of each row:')\n",
    "print(torch.sum(x, dim=0).shape)\n",
    "\n",
    "# Sum over each column:\n",
    "print('\\nSum of each column:')\n",
    "print(torch.sum(x, dim=1))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-soundtrack",
   "metadata": {},
   "source": [
    "#### Matrix Operations\n",
    "\n",
    "- `@` is used for multiplication, same as `torch.mm()`, `torch.mv()`, `torch.bmm()`, `torch.bmv()`\n",
    "- `torch.matmul()`\n",
    "    - If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "    - If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "    - If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n",
    "    - If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n",
    "    - Support broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dangerous-platinum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x0 shape: torch.Size([])\n",
      "\n",
      " x1 shape: torch.Size([2, 3])\n",
      "\n",
      " x2 shape: torch.Size([2])\n",
      "\n",
      " x3 shape: torch.Size([2, 3])\n",
      "\n",
      " x4 shape: torch.Size([10, 3, 5])\n",
      "\n",
      " vector x vector torch.Size([])\n",
      " matrix x vector torch.Size([3])\n",
      " batched matrix x broadcasted vector torch.Size([10, 3])\n",
      " batched matrix x broadcasted vector torch.Size([10, 3, 5])\n",
      " batched matrix x broadcasted vector torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Operations\n",
    "import torch\n",
    "\n",
    "# torch.dot\n",
    "x0 = torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\n",
    "print(f'\\n x0 shape: {x0.shape}')\n",
    "\n",
    "# torch.mm\n",
    "x1 = torch.mm(torch.randn(2, 3), torch.randn(3, 3))\n",
    "print(f'\\n x1 shape: {x1.shape}')\n",
    "\n",
    "# torch.mv\n",
    "x2 = torch.mv(torch.randn(2, 3), torch.randn(3))\n",
    "print(f'\\n x2 shape: {x2.shape}')\n",
    "\n",
    "# torch.admm\n",
    "M = torch.randn(2, 3)\n",
    "m1 = torch.randn(2, 3)\n",
    "m2 = torch.randn(3, 3)\n",
    "# m1 @ m2 + M\n",
    "x3 = torch.addmm(M, m1, m2)\n",
    "print(f'\\n x3 shape: {x3.shape}')\n",
    "\n",
    "# torch.bmm\n",
    "bm1 = torch.randn(10, 3, 4)\n",
    "bm2 = torch.randn(10, 4, 5)\n",
    "x4 = torch.bmm(bm1, bm2)\n",
    "print(f'\\n x4 shape: {x4.shape}')\n",
    "\n",
    "# torch.matmul()\n",
    "# vector x vector\n",
    "print(f'\\n vector x vector {torch.matmul(torch.randn(3), torch.randn(3)).size()}')\n",
    "# matrix x vector\n",
    "print(f' matrix x vector {torch.matmul(torch.randn(3, 4), torch.randn(4)).size()}')\n",
    "# batched matrix x broadcasted vector\n",
    "print(f' batched matrix x broadcasted vector {torch.matmul(torch.randn(10, 3, 4), torch.randn(4)).size()}')\n",
    "# batched matrix x batched matrix\n",
    "print(f' batched matrix x broadcasted vector {torch.matmul(torch.randn(10, 3, 4), torch.randn(10, 4, 5)).size()}')\n",
    "# batched matrix x broadcasted matrix\n",
    "print(f' batched matrix x broadcasted vector {torch.matmul(torch.randn(10, 3, 4), torch.randn(4, 5)).size()}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-radar",
   "metadata": {},
   "source": [
    "#### Tensor Broadcasting\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "metropolitan-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is x (before broadcasting):\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "x.shape:  torch.Size([4, 3])\n",
      "\n",
      "Here is v (before broadcasting):\n",
      "tensor([1, 0, 1])\n",
      "v.shape:  torch.Size([3])\n",
      "\n",
      "Here is xx (after) broadcasting):\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "xx.shape:  torch.Size([4, 3])\n",
      "\n",
      "Here is vv (after broadcasting):\n",
      "tensor([[1, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [1, 0, 1]])\n",
      "vv.shape:  torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "v = torch.tensor([1, 0, 1])\n",
    "print('Here is x (before broadcasting):')\n",
    "print(x)\n",
    "print('x.shape: ', x.shape)\n",
    "print('\\nHere is v (before broadcasting):')\n",
    "print(v)\n",
    "print('v.shape: ', v.shape)\n",
    "\n",
    "xx, vv = torch.broadcast_tensors(x, v)\n",
    "print('\\nHere is xx (after) broadcasting):')\n",
    "print(xx)\n",
    "print('xx.shape: ', x.shape)\n",
    "print('\\nHere is vv (after broadcasting):')\n",
    "print(vv)\n",
    "print('vv.shape: ', vv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-seattle",
   "metadata": {},
   "source": [
    "### Tensor Data Type\n",
    "<code>\n",
    "    tensor.to(),\n",
    "    tensor.new_zeros(),\n",
    "    tensor.float(),\n",
    "    tensor.double(),\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "blind-romantic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype when torch chooses for us:\n",
      "List of integers: torch.int64\n",
      "List of floats: torch.float32\n",
      "Mixed list: torch.float32\n",
      "\n",
      "dtype when we force a datatype:\n",
      "32-bit float:  torch.float32\n",
      "32-bit integer:  torch.int32\n",
      "64-bit integer:  torch.int64\n",
      "\n",
      "torch.ones with different dtypes\n",
      "default dtype: torch.float32\n",
      "16-bit integer: torch.int16\n",
      "8-bit unsigned integer: torch.uint8\n",
      "\n",
      "x0: torch.int64\n",
      "x1: torch.float32\n",
      "x2: torch.float64\n",
      "x3: torch.float32\n",
      "x4: torch.float64\n",
      "\n",
      "x0 shape is torch.Size([3, 3]), dtype is torch.float64\n",
      "x1 shape is torch.Size([3, 3]), dtype is torch.float64\n",
      "x2 shape is torch.Size([4, 5]), dtype is torch.float64\n",
      "x3 shape is torch.Size([6, 7]), dtype is torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Tensor Data Type\n",
    "\n",
    "# Let torch choose the datatype\n",
    "x0 = torch.tensor([1, 2])   # List of integers\n",
    "x1 = torch.tensor([1., 2.]) # List of floats\n",
    "x2 = torch.tensor([1., 2])  # Mixed list\n",
    "print('dtype when torch chooses for us:')\n",
    "print('List of integers:', x0.dtype)\n",
    "print('List of floats:', x1.dtype)\n",
    "print('Mixed list:', x2.dtype)\n",
    "\n",
    "# Force a particular datatype\n",
    "y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n",
    "y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n",
    "y2 = torch.tensor([1, 2], dtype=torch.int64)    # 64-bit (signed) integer\n",
    "print('\\ndtype when we force a datatype:')\n",
    "print('32-bit float: ', y0.dtype)\n",
    "print('32-bit integer: ', y1.dtype)\n",
    "print('64-bit integer: ', y2.dtype)\n",
    "\n",
    "# Other creation ops also take a dtype argument\n",
    "z0 = torch.ones(1, 2)  # Let torch choose for us\n",
    "z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n",
    "z2 = torch.ones(1, 2, dtype=torch.uint8) # 8-bit (unsigned) integer\n",
    "print('\\ntorch.ones with different dtypes')\n",
    "print('default dtype:', z0.dtype)\n",
    "print('16-bit integer:', z1.dtype)\n",
    "print('8-bit unsigned integer:', z2.dtype)\n",
    "\n",
    "x0 = torch.eye(3, dtype=torch.int64)\n",
    "x1 = x0.float()  # Cast to 32-bit float\n",
    "x2 = x0.double() # Cast to 64-bit float\n",
    "x3 = x0.to(torch.float32) # Alternate way to cast to 32-bit float\n",
    "x4 = x0.to(torch.float64) # Alternate way to cast to 64-bit float\n",
    "print('\\nx0:', x0.dtype)\n",
    "print('x1:', x1.dtype)\n",
    "print('x2:', x2.dtype)\n",
    "print('x3:', x3.dtype)\n",
    "print('x4:', x4.dtype)\n",
    "\n",
    "x0 = torch.eye(3, dtype=torch.float64)  # Shape (3, 3), dtype torch.float64\n",
    "x1 = torch.zeros_like(x0)               # Shape (3, 3), dtype torch.float64\n",
    "x2 = x0.new_zeros(4, 5)                 # Shape (4, 5), dtype torch.float64\n",
    "x3 = torch.ones(6, 7).to(x0)            # Shape (6, 7), dtype torch.float64)\n",
    "print('\\nx0 shape is %r, dtype is %r' % (x0.shape, x0.dtype))\n",
    "print('x1 shape is %r, dtype is %r' % (x1.shape, x1.dtype))\n",
    "print('x2 shape is %r, dtype is %r' % (x2.shape, x2.dtype))\n",
    "print('x3 shape is %r, dtype is %r' % (x3.shape, x3.dtype))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-prophet",
   "metadata": {},
   "source": [
    "### Tensor Reshape\n",
    "<code>\n",
    "    tensor.view(), tensor.reshape(), tensor.transpose(), tensor.permute(), tensor.contiguous()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-electronics",
   "metadata": {},
   "source": [
    "The `view()` function takes elements in row-major order, so you cannot transpose matrices with `view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ancient-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "shape: torch.Size([2, 4])\n",
      "\n",
      "Flattened tensor:\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "shape: torch.Size([8])\n",
      "\n",
      "Rank 2 tensor:\n",
      "shape: torch.Size([8, 1])\n",
      "\n",
      "Rank 3 tensor:\n",
      "shape: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# tensor.view()\n",
    "\n",
    "x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "print('Original tensor:')\n",
    "print(x0)\n",
    "print('shape:', x0.shape)\n",
    "\n",
    "# Flatten x0 into a rank 1 vector of shape (8,)\n",
    "# Takes elements in row-major order\n",
    "x1 = x0.view(-1)\n",
    "print('\\nFlattened tensor:')\n",
    "print(x1)\n",
    "print('shape:', x1.shape)\n",
    "\n",
    "# Rank 2 tensor\n",
    "x2 = x0.view(-1, 1)\n",
    "print('\\nRank 2 tensor:')\n",
    "print('shape:', x2.shape)\n",
    "\n",
    "# View x1 as shape (2, 2, 2)\n",
    "x3 = x0.view(-1, 2, 2)\n",
    "print('\\nRank 3 tensor:')\n",
    "print('shape:', x3.shape)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "laden-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Swap axes 1 and 2:\n",
      "tensor([[[ 1,  5,  9],\n",
      "         [ 2,  6, 10],\n",
      "         [ 3,  7, 11],\n",
      "         [ 4,  8, 12]],\n",
      "\n",
      "        [[13, 17, 21],\n",
      "         [14, 18, 22],\n",
      "         [15, 19, 23],\n",
      "         [16, 20, 24]]])\n",
      "torch.Size([2, 4, 3])\n",
      "\n",
      "Permute axes\n",
      "tensor([[[ 1, 13],\n",
      "         [ 2, 14],\n",
      "         [ 3, 15],\n",
      "         [ 4, 16]],\n",
      "\n",
      "        [[ 5, 17],\n",
      "         [ 6, 18],\n",
      "         [ 7, 19],\n",
      "         [ 8, 20]],\n",
      "\n",
      "        [[ 9, 21],\n",
      "         [10, 22],\n",
      "         [11, 23],\n",
      "         [12, 24]]])\n",
      "shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Tensor transpose and permute\n",
    "\n",
    "# Create a tensor of shape (2, 3, 4)\n",
    "x0 = torch.tensor([\n",
    "     [[1,  2,  3,  4],\n",
    "      [5,  6,  7,  8],\n",
    "      [9, 10, 11, 12]],\n",
    "     [[13, 14, 15, 16],\n",
    "      [17, 18, 19, 20],\n",
    "      [21, 22, 23, 24]]])\n",
    "print('Original tensor:')\n",
    "print(x0)\n",
    "print('shape:', x0.shape)\n",
    "\n",
    "# Swap axes 1 and 2; shape is (2, 4, 3)\n",
    "x1 = x0.transpose(1, 2)\n",
    "print('\\nSwap axes 1 and 2:')\n",
    "print(x1)\n",
    "print(x1.shape)\n",
    "\n",
    "# Permute axes; the argument (1, 2, 0) means:\n",
    "# - Make the old dimension 1 appear at dimension 0;\n",
    "# - Make the old dimension 2 appear at dimension 1;\n",
    "# - Make the old dimension 0 appear at dimension 2\n",
    "# This results in a tensor of shape (3, 4, 2)\n",
    "x2 = x0.permute(1, 2, 0)\n",
    "print('\\nPermute axes')\n",
    "print(x2)\n",
    "print('shape:', x2.shape)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-sense",
   "metadata": {},
   "source": [
    "### Tensor Test\n",
    "<code>\n",
    "    torch.all(), torch.is_close(), torch.all_close()\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-millennium",
   "metadata": {},
   "source": [
    "### Tensor GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cubic-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 device: cpu\n",
      "Cuda is not available.\n"
     ]
    }
   ],
   "source": [
    "# Tensor on GPU\n",
    "\n",
    "# Construct a tensor on the CPU\n",
    "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print('x0 device:', x0.device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Move it to the GPU using .to()\n",
    "    x1 = x0.to('cuda')\n",
    "    print('x1 device:', x1.device)\n",
    "\n",
    "    # Move it to the GPU using .cuda()\n",
    "    x2 = x0.cuda()\n",
    "    print('x2 device:', x2.device)\n",
    "\n",
    "    # Move it back to the CPU using .to()\n",
    "    x3 = x1.to('cpu')\n",
    "    print('x3 device:', x3.device)\n",
    "\n",
    "    # Move it back to the CPU using .cpu()\n",
    "    x4 = x2.cpu()\n",
    "    print('x4 device:', x4.device)\n",
    "\n",
    "    # We can construct tensors directly on the GPU as well\n",
    "    y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
    "    print('y device / dtype:', y.device, y.dtype)\n",
    "\n",
    "    # Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
    "    # device and dtype as y\n",
    "    x5 = x0.to(y)\n",
    "    print('x5 device / dtype:', x5.device, x5.dtype)\n",
    "else:\n",
    "    print('Cuda is not available.' )\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-macintosh",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "<code>\n",
    "    tensor.requires_grad_(), tensor.detach() \n",
    "</code>\n",
    "\n",
    "Every Tensor has a flag: `requires_grad` that allows for fine grained exclusion of subgraphs from gradient computation and can increase efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "valued-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a requires_grad: False\n",
      "b requires_grad: True\n",
      "z grad: \n",
      " tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch autograd\n",
    "\n",
    "import torch\n",
    "x = torch.randn(5, 5)  # requires_grad=False by default\n",
    "y = torch.randn(5, 5)  # requires_grad=False by default\n",
    "z = torch.randn((5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "print(f'a requires_grad: {a.requires_grad}')\n",
    "b = (a + 2*z).sum()\n",
    "print(f'b requires_grad: {b.requires_grad}')\n",
    "b.backward()\n",
    "print(f'z grad: \\n {z.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-geneva",
   "metadata": {},
   "source": [
    "## Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-chinese",
   "metadata": {},
   "source": [
    "## Pytorch Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-jackson",
   "metadata": {},
   "source": [
    "## Pytorch Loss Functions\n",
    "<code>\n",
    "   torch.nn.MSELoss(), torch.nn.CrossEntropyLoss(),torch.nn.MultiLabelMarginLoss(), \n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-harassment",
   "metadata": {},
   "source": [
    "### MSELoss()\n",
    "- Input: (N, )\n",
    "- Target: (N, )\n",
    "- Output: Scalar. If reduction is 'none', then (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bottom-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "import torch\n",
    "loss = torch.nn.MSELoss()\n",
    "x = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(x, target)\n",
    "output.backward()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-sarah",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss()\n",
    "- Input: (N, C), N is batch size, C is number of classes\n",
    "- Target: (N,), $0 \\leq target[i] \\leq C-1 $\n",
    "- Outtput: (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "southern-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "\n",
    "import torch\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "x = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(x, target)\n",
    "output.backward()\n",
    "\n",
    "# Image pixel level classifiction for 5 classes\n",
    "width = 10\n",
    "height = 10\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "x = torch.randn(3, 5, width, height, requires_grad=True)\n",
    "target = torch.empty(3, width, height, dtype=torch.long).random_(5)\n",
    "output = loss(x, target)\n",
    "output.backward()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-ordinance",
   "metadata": {},
   "source": [
    "### MultiLabelMarginLoss()\n",
    "Creates a criterion that optimizes a multi-class multi-classification hinge loss. This means that for a sample x, it could have multiple correct labels. \n",
    "- Input: (C,) or (N, C),  N is batch size, C is number of classes\n",
    "- Target: (C) or (N, C), label targets after first -1 are ignored\n",
    "- Output: Scalar. If reduction is 'none', then (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "healthy-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single class loss: 0.3250\n",
      "multi-class loss: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "import torch\n",
    "loss = torch.nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])\n",
    "# Single class hinge loss, so label == 3\n",
    "y = torch.LongTensor([[3, -1, -1, -1]])\n",
    "output = loss(x, y)\n",
    "print(f'single class loss: {output.item():.4f}')\n",
    "expected = torch.tensor([0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))])\n",
    "assert(torch.isclose(output, expected))\n",
    "\n",
    "# Multi-class hinge loss, so label == 3 and label == 1\n",
    "y = torch.LongTensor([[3, 0, -1, -1]])\n",
    "output = loss(x, y)\n",
    "print(f'multi-class loss: {output.item():.4f}')\n",
    "expected = torch.tensor([0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))])\n",
    "assert(torch.isclose(output, expected))\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
