{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIQD-7Q2ExL5"
   },
   "source": [
    "# Hand-Eye Calibration Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXhVakwGHgFn"
   },
   "source": [
    "## Frame Definitions and Observed Points\n",
    "\n",
    "In the hand-eye calibration problem, we define the following coordinate frames:\n",
    "\n",
    "- **Base Frame**: The fixed coordinate frame of the robot arm, denoted as $ \\mathbf{B} $\n",
    "- **End-Effector Frame**: The frame attached to the robot's end-effector, denoted as $ \\mathbf{E} $\n",
    "- **Camera Frame**: The coordinate frame of the camera, which is rigidly mounted to the end-effector, denoted as $ \\mathbf{C} $\n",
    "- **World Frame**: A global reference frame in which known points are defined, denoted as $ \\mathbf{W} $\n",
    "\n",
    "We assume that four known points are in the world frame:\n",
    "\n",
    "$$\n",
    "p_W = \\begin{bmatrix}\n",
    "-1 & -1 & 0 \\\\\n",
    "-1 & 1 & 0\\\\\n",
    "1 & -1 & 0\\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each row represents the \\((x, y, z)\\) coordinates of a point in the world frame.\n",
    "\n",
    "These points serve as reference points that can be used to estimate the transformation between different coordinate frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG2msTEM42HM"
   },
   "source": [
    "## Hand-Eye Calibration Goal\n",
    "\n",
    "In the hand-eye calibration problem, our goal is to estimate the transformation between the camera and the end-effector, as well as the camera's intrinsic parameters.\n",
    "\n",
    "We define the transformation from the **camera frame** $\\mathbf{C}$ to the **end-effector frame** $\\mathbf{E}$ as: $\\mathbf{T}_{EC} \\in SE(3)$. This transformation allows us to convert a point $\\mathbf{p}_C$ in the camera frame to the end-effector frame using: $\\mathbf{p}_E = \\mathbf{T}_{EC} \\mathbf{p}_C$\n",
    "\n",
    "In addition to the hand-eye transformation, we may also need to estimate the **camera intrinsics**, which describe how 3D points in the camera frame are projected onto the image plane. The camera intrinsics matrix is given by\n",
    "$ \\mathbf{K} =\n",
    "\\begin{bmatrix}\n",
    "f_x & \\gamma & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "where:\n",
    "\n",
    "- $f_x, f_y$ are the focal lengths in pixels,\n",
    "- $c_x, c_y$ are the principal point coordinates.\n",
    "- $\\gamma$ is a skew parameter.\n",
    "\n",
    "Note:\n",
    "- In this tutorial, we assume there is no tangent and radial distortion\n",
    "- See `Optimal Hand-Eye Calibration` from Klaus H Strobl and Gerd Hirzinger for more details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrcLA-0pE4mB"
   },
   "source": [
    "## Install packates (on Google Colab) and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7HFun7QpiH_"
   },
   "outputs": [],
   "source": [
    "# Run this first if you use GoogleColab\n",
    "\n",
    "!pip install pytransform3d\n",
    "\n",
    "# You don't need this if you don't want to run factor-graph based opimization.\n",
    "!pip install symforce\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1738461164132,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "HB-SCFke8HKg"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "from pytransform3d import transformations as pt\n",
    "from pytransform3d import rotations as pr\n",
    "import random\n",
    "import copy\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb3tq3J8FA4q"
   },
   "source": [
    "# Steps to Solve the Hand-Eye Calibration Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL1NIu-dphIK"
   },
   "source": [
    "## References:\n",
    "\n",
    "`Robot Sensor Calibration: Solving AX = XB on the Euclidean Group` Frank C. Park and Bryan J Martin\n",
    "\n",
    "`Optimal Hand-Eye Calibration` Klaus H Strobl and Gerd Hirzinger\n",
    "\n",
    "`A flexible new technique for camera calibration` Zhengyou Zhang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foRXM0d8FxI1"
   },
   "source": [
    "## Simulated Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgfQLGSYGCJL"
   },
   "source": [
    "To validate the hand-eye calibration method, we define a simulated environment where a camera observes known 3D points in the world. This setup allows us to generate synthetic camera poses and project world points onto the image plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpSFozUtGHKJ"
   },
   "source": [
    "### Defining 3D Points in the World Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny7vNjQwGQlp"
   },
   "source": [
    "We define four 3D points in the **world frame** $\\mathbf{W}$ that lie on the $z=0$ plane:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_W =\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & 0 \\\\\n",
    "1 & -1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "-1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To work with homogeneous coordinates, we append a row of ones:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{p}}_W =\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 & 1 & -1 \\\\\n",
    "-1 & -1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCmRxyQXGUX1"
   },
   "source": [
    "### Camera Intrinsic Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFwGyvqcGZl5"
   },
   "source": [
    "The camera intrinsics matrix $\\mathbf{K}$ models how 3D points are projected onto the image plane:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} =\n",
    "\\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $f_x = 800, f_y = 800$ are the focal lengths (in pixels),\n",
    "- $c_x = 320, c_y = 240$ are the principal point coordinates.\n",
    "- We have $\\gamma$ as 0 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWJrb6hOGagC"
   },
   "source": [
    "### Generating Camera Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgWmgKsq9iuM"
   },
   "source": [
    "\n",
    "\n",
    "To simulate multiple views, we generate camera poses in spherical coordinates, where:\n",
    "\n",
    "- The camera moves along a sphere of varying radii ($r \\in [10, 20]$),\n",
    "- The **azimuth angle** varies from $0^\\circ$ to $360^\\circ$,\n",
    "- The **elevation angle** varies from $30^\\circ$ to $60^\\circ$.\n",
    "\n",
    "For a given azimuth $\\theta$ and elevation $\\phi$, the camera position in Cartesian coordinates is:\n",
    "\n",
    "$$\n",
    "\\mathbf{t} =\n",
    "\\begin{bmatrix}\n",
    "r \\cos\\phi \\cos\\theta \\\\\n",
    "r \\cos\\phi \\sin\\theta \\\\\n",
    "r \\sin\\phi\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The camera orientation is determined using a **look-at transformation**, where:\n",
    "\n",
    "- The **forward vector** points from the camera to the world origin,\n",
    "- The **right vector** is computed as the cross-product of the world $z$-axis and the forward vector,\n",
    "- The **up vector** is computed as the cross-product of the forward and right vectors.\n",
    "\n",
    "The rotation matrix $\\mathbf{R}_{WC}$ is then constructed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{WC} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{r} & \\mathbf{u} & \\mathbf{f}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{r}, \\mathbf{u}, \\mathbf{f}$ are the right, up, and forward unit vectors, respectively.\n",
    "\n",
    "The final transformation from the world to the camera frame is:\n",
    "\n",
    "$$\n",
    "\\mathbf{T}_{WC} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{R}_{WC} & \\mathbf{t} \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This process generates a set of camera poses that will be used to simulate observations and test the hand-eye calibration process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1738471288561,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "nqfqWGOzphIL"
   },
   "outputs": [],
   "source": [
    "# Generate a simulated problem\n",
    "\n",
    "# Define the 3D points in the world coordinate system\n",
    "world_points = np.array([[-1, -1, 0], [1, -1, 0], [1, 1, 0], [-1, 1, 0]])\n",
    "homogeneous_world_points = np.vstack((world_points.T, np.ones(world_points.shape[0])))\n",
    "\n",
    "# Define the camera intrinsic matrix (K)\n",
    "fx, fy = 800, 800  # Focal lengths\n",
    "cx, cy = 320, 240  # Principal point\n",
    "K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "\n",
    "def generate_camera_poses(num_r=2, num_z=4, num_e=4):\n",
    "    # def generate_camera_poses(num_r=4, num_z=10, num_e=10):\n",
    "    camera_poses = []\n",
    "\n",
    "    for r in np.linspace(10, 20, num=num_r):\n",
    "        for azimuth_deg in np.linspace(0, 360, num=num_z):\n",
    "            for elevation_deg in np.linspace(30, 60, num=num_e):\n",
    "                azimuth = np.radians(azimuth_deg)\n",
    "                elevation = np.radians(elevation_deg)\n",
    "\n",
    "                # Convert spherical to Cartesian coordinates\n",
    "                x = r * np.cos(elevation) * np.cos(azimuth)\n",
    "                y = r * np.cos(elevation) * np.sin(azimuth)\n",
    "                z = r * np.sin(elevation)\n",
    "\n",
    "                # Camera position\n",
    "                t = np.array([x, y, z])\n",
    "\n",
    "                # Camera rotation matrix\n",
    "                # Note: Camera is looking at its negative z\n",
    "                forward = -t / np.linalg.norm(t)  # z\n",
    "                right = np.cross([0, 0, 1], forward)  # x\n",
    "                right /= np.linalg.norm(right)\n",
    "                up = np.cross(forward, right)  # y\n",
    "                R = np.vstack([right, up, forward]).T\n",
    "\n",
    "                # Construct transformation matrix\n",
    "                T_W_C = pt.transform_from(R, t)\n",
    "                camera_poses.append(T_W_C)\n",
    "    return camera_poses\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79nPjw07GmST"
   },
   "source": [
    "## Camera Intrinsic and Extrinsic Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMHW7EqGGpCv"
   },
   "source": [
    "To estimate both the camera intrinsics and extrinsics, we apply **Zhang's method** [`A flexible new technique for camera calibration`], which involves the following steps:\n",
    "\n",
    "1. **Estimating Homographies**: We compute the transformation between the world and image planes for multiple camera views.\n",
    "2. **Calibrating Camera Intrinsics**: Using homographies, we solve for the camera intrinsic matrix $\\mathbf{K}$.\n",
    "3. **Estimating Camera Extrinsics**: Given $\\mathbf{K}$ and the homographies, we recover the camera pose for each view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP4j66wFGq1X"
   },
   "source": [
    "### Homography Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TI24UikBXr8"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "A **homography** is a $3 \\times 3$ matrix $\\mathbf{H}$ that describes the transformation between two planes in projective space. In the context of camera calibration, it maps **3D world points** that lie on a known plane (e.g., the $Z = 0$ plane in our simulated problem) to their corresponding **2D image points**:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{p}}_I = \\mathbf{H} \\tilde{\\mathbf{p}}_W\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\tilde{\\mathbf{p}}_W = (x, y, 1)^\\top$ is the homogeneous coordinate of a point in the world frame.\n",
    "- $\\tilde{\\mathbf{p}}_I = (u, v, 1)^\\top$ is the corresponding image point in homogeneous coordinates.\n",
    "- $\\mathbf{H}$ is the $3 \\times 3$ homography matrix.\n",
    "\n",
    "Since all world points lie on the **$Z=0$ plane**, their 3D coordinates simplify to:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_W = (x, y, 0)^\\top\n",
    "$$\n",
    "\n",
    "The camera projection equation is given by:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{p}}_I = \\mathbf{K} [\\mathbf{R} \\ | \\ \\mathbf{t}] \\tilde{\\mathbf{p}}_W\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{K}$ is the **intrinsic matrix**, which defines how 3D points are projected onto the image plane.\n",
    "- $[\\mathbf{R} \\ | \\ \\mathbf{t}]$ is the **extrinsic transformation**, with $\\mathbf{R} \\in SO(3)$ as the rotation matrix and $\\mathbf{t} \\in \\mathbb{R}^3$ as the translation vector.\n",
    "- Since world points are on the $Z=0$ plane, the third column of $\\mathbf{R}$ is irrelevant in this mapping.\n",
    "\n",
    "Thus, we can rewrite the projection equation by extracting only the first two columns of $\\mathbf{R}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\mathbf{K} [\\mathbf{r}_1 \\quad \\mathbf{r}_2 \\quad \\mathbf{t}]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{r}_1$ and $\\mathbf{r}_2$ are the first two columns of the rotation matrix $\\mathbf{R}$.\n",
    "- $\\mathbf{t}$ is the translation vector.\n",
    "- The homography $\\mathbf{H}$ encodes both the **intrinsic** ($\\mathbf{K}$) and **extrinsic** ($\\mathbf{R}, \\mathbf{t}$) parameters of the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBrtbAEnI5uj"
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACSe1SuuEkmA"
   },
   "source": [
    "\n",
    "To improve numerical stability, we normalize the points to be on unit circle before estimating the homography. This process ensures that the coordinate values are well-conditioned for numerical computations.\n",
    "\n",
    "Given a set of points $\\mathbf{p}$, we compute a normalization transformation matrix $\\mathbf{T}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{T} =\n",
    "\\begin{bmatrix}\n",
    "s & 0 & -s \\bar{x} \\\\\n",
    "0 & s & -s \\bar{y} \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Before estimating the homography, we apply this normalization process separately to the **world points** and **image points**, yielding two transformation matrices:\n",
    "\n",
    "The transformation matrix $\\mathbf{T}_{\\text{world}}$ normalizes the **2D world points** $(x, y)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{T}_{\\text{world}} =\n",
    "\\begin{bmatrix}\n",
    "s_W & 0 & -s_W \\bar{x}_W \\\\\n",
    "0 & s_W & -s_W \\bar{y}_W \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying this transformation:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{p}}_W' = \\mathbf{T}_{\\text{world}} \\tilde{\\mathbf{p}}_W\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly, the transformation matrix $\\mathbf{T}_{\\text{pixel}}$ normalizes the **2D image points** $(u, v)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{T}_{\\text{pixel}} =\n",
    "\\begin{bmatrix}\n",
    "s_I & 0 & -s_I \\bar{u} \\\\\n",
    "0 & s_I & -s_I \\bar{v} \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying this transformation:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{p}}_I' = \\mathbf{T}_{\\text{pixel}} \\tilde{\\mathbf{p}}_I\n",
    "$$\n",
    "\n",
    "\n",
    "After applying these transformations, the normalized world points $\\tilde{\\mathbf{p}}_W'$ and image points $\\tilde{\\mathbf{p}}_I'$ are used to compute the homography $\\mathbf{H}_{\\text{norm}}$. The final homography is then **denormalized**:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\mathbf{T}_{\\text{pixel}}^{-1} \\mathbf{H}_{\\text{norm}} \\mathbf{T}_{\\text{world}}\n",
    "$$\n",
    "\n",
    "This process ensures better numerical stability in solving for the homography using **singular value decomposition (SVD)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1738471290663,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "jmhXKnXqJAva"
   },
   "outputs": [],
   "source": [
    "# Point Normalization\n",
    "\n",
    "def normalize_points(points):\n",
    "    \"\"\"Normalize points to have zero mean and unit variance.\"\"\"\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std_dev = np.std(points)\n",
    "    scale = np.sqrt(2) / std_dev\n",
    "    T = np.array(\n",
    "        [[scale, 0, -scale * mean[0]], [0, scale, -scale * mean[1]], [0, 0, 1]]\n",
    "    )\n",
    "    normalized_points = (T @ np.vstack((points.T, np.ones(points.shape[0]))))[:2].T\n",
    "    return normalized_points, T\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIyGKcICJKc7"
   },
   "source": [
    "### DLT for homography estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZATtDGiEiXo"
   },
   "source": [
    "Using normalized world points $\\mathbf{p}_W$ and image points $\\mathbf{p}_I$, we construct a **direct linear transformation (DLT)** system.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "h_{11} & h_{12} & h_{13} \\\\\n",
    "h_{21} & h_{22} & h_{23} \\\\\n",
    "h_{31} & h_{32} & h_{33}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "u = \\frac{h_{11} x + h_{12} y + h_{13}}{h_{31} x + h_{32} y + h_{33}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = \\frac{h_{21} x + h_{22} y + h_{23}}{h_{31} x + h_{32} y + h_{33}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "h_{11} x + h_{12} y + h_{13} - u h_{31} x - u h_{32} y - u h_{33} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{21} x + h_{22} y + h_{23} - v h_{31} x - v h_{32} y - v h_{33} = 0\n",
    "$$\n",
    "\n",
    "Each point correspondence $(x, y) \\rightarrow (u, v)$ contributes two equations. To solve for all elements of $\\mathbf{H}$, we stack these equations for multiple points into a **linear system**:\n",
    "\n",
    "$$\n",
    "A \\mathbf{h} = 0\n",
    "$$\n",
    "\n",
    "where $\\mathbf{h}$ is the 9-element vector.\n",
    "\n",
    "The **system matrix** $A$ for each point correspondence $(x, y) \\rightarrow (u, v)$ is:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "x & y & 1 & 0 & 0 & 0 & -u x & -u y & -u \\\\\n",
    "0 & 0 & 0 & x & y & 1 & -v x & -v y & -v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By stacking these equations for multiple points and solving the **homogeneous linear system** using **singular value decomposition (SVD)**, we obtain the best estimate for $\\mathbf{H}$.\n",
    "\n",
    "Note: Since `H` has 8 DoF, at least 4 points are required to solve this equation, as each point provides 2 constraints. The points must not be collinear, as two points alone would determine the third, making the solution underdetermined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1738471290898,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "elOKK8o1JRpj"
   },
   "outputs": [],
   "source": [
    "# Homography estimation\n",
    "\n",
    "\n",
    "def estimate_homography(world_points, pixel_points):\n",
    "    \"\"\"Estimate homography using normalized points.\"\"\"\n",
    "    world_norm, T_world = normalize_points(world_points[:, :2])\n",
    "    pixel_norm, T_pixel = normalize_points(pixel_points.T)\n",
    "\n",
    "    A = []\n",
    "    for (x, y), (u, v) in zip(world_norm, pixel_norm):\n",
    "        A.append([x, y, 1, 0, 0, 0, -u * x, -u * y, -u])\n",
    "        A.append([0, 0, 0, x, y, 1, -v * x, -v * y, -v])\n",
    "\n",
    "    A = np.array(A)\n",
    "\n",
    "    # Perform SVD and take the last row of V^T\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    H_norm = Vt[-1].reshape(3, 3)\n",
    "\n",
    "    # Denormalize the homography matrix\n",
    "    H = np.linalg.inv(T_pixel) @ H_norm @ T_world\n",
    "\n",
    "    # Normalize so that H[2,2] = 1\n",
    "    H /= H[2, 2]\n",
    "\n",
    "    return H\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpOyxqczPc-1"
   },
   "source": [
    "### Camera Intrinsic Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc1KH7pYPfMI"
   },
   "source": [
    "Once multiple homographies $\\mathbf{H}$ are computed, we estimate the camera intrinsic matrix $\\mathbf{K}$.\n",
    "\n",
    "The homography relates to intrinsics and extrinsics:\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\mathbf{K} [ \\mathbf{r}_1 \\quad \\mathbf{r}_2 \\quad \\mathbf{t} ]\n",
    "$$\n",
    "\n",
    "where $\\mathbf{r}_1, \\mathbf{r}_2$ are the first two columns of the rotation matrix, and $\\mathbf{t}$ is the translation vector.\n",
    "\n",
    "---\n",
    "\n",
    "**- Constraint 1: Orthogonality of $\\mathbf{r}_1$ and $\\mathbf{r}_2$**  \n",
    "The rotation columns must be **orthogonal**, meaning:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_1^\\top \\mathbf{r}_2 = 0\n",
    "$$\n",
    "\n",
    "Expanding in terms of $\\mathbf{H}$:\n",
    "\n",
    "$$\n",
    "(\\mathbf{K}^{-1} \\mathbf{h}_1)^\\top (\\mathbf{K}^{-1} \\mathbf{h}_2) = 0\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^\\top \\mathbf{K}^{-\\top} \\mathbf{K}^{-1} \\mathbf{h}_2 = 0\n",
    "$$\n",
    "\n",
    "**- Constraint 2: Equality of Norms of $\\mathbf{r}_1$ and $\\mathbf{r}_2$**  \n",
    "Since the columns of a rotation matrix have unit norms:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_1^\\top \\mathbf{r}_1 = \\mathbf{r}_2^\\top \\mathbf{r}_2\n",
    "$$\n",
    "\n",
    "Expanding in terms of $\\mathbf{H}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^\\top \\mathbf{K}^{-\\top} \\mathbf{K}^{-1} \\mathbf{h}_1 = \\mathbf{h}_2^\\top \\mathbf{K}^{-\\top} \\mathbf{K}^{-1} \\mathbf{h}_2\n",
    "$$\n",
    "\n",
    "These two constraints hold for each homography and will allow us to estimate $\\mathbf{K}$.\n",
    "\n",
    "---\n",
    "\n",
    "To simplify notation, we define:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{K}^{-\\top} \\mathbf{K}^{-1}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{B}$ is **symmetric ($B = B^T$)**, it has the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "B_{00} & B_{01} & B_{02} \\\\\n",
    "B_{01} & B_{11} & B_{12} \\\\\n",
    "B_{02} & B_{12} & B_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Substituting $\\mathbf{B}$ into our constraints:\n",
    "\n",
    "1. **Orthogonality Condition:**\n",
    "   \n",
    "   $$\n",
    "   \\mathbf{h}_1^\\top \\mathbf{B} \\mathbf{h}_2 = 0\n",
    "   $$\n",
    "\n",
    "2. **Equality of Norms Condition:**\n",
    "   \n",
    "   $$\n",
    "   \\mathbf{h}_1^\\top \\mathbf{B} \\mathbf{h}_1 = \\mathbf{h}_2^\\top \\mathbf{B} \\mathbf{h}_2\n",
    "   $$\n",
    "\n",
    "These provide two **linear equations** for each homography $\\mathbf{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rviWyUHtTYlo"
   },
   "source": [
    "Let the vector $\\mathbf{b}$ represents the **six unknown elements** of the intrinsic constraint matrix $\\mathbf{B}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{b} =\n",
    "\\begin{bmatrix}\n",
    "B_{00} \\\\ B_{01} \\\\ B_{11} \\\\ B_{02} \\\\ B_{12} \\\\ B_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The term \\( v_{12} \\) is derived from the **orthogonality constraint** of the rotation columns:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^\\top \\mathbf{B} \\mathbf{h}_2 = 0\n",
    "$$\n",
    "\n",
    "Expanding this gives the constraint vector:\n",
    "\n",
    "$$\n",
    "v_{12} =\n",
    "\\begin{bmatrix}\n",
    "h_{11} h_{12} \\\\\n",
    "h_{11} h_{22} + h_{21} h_{12} \\\\\n",
    "h_{21} h_{22} \\\\\n",
    "h_{11} h_{32} + h_{31} h_{12} \\\\\n",
    "h_{21} h_{32} + h_{31} h_{22} \\\\\n",
    "h_{31} h_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which contributes one **row** in the constraint matrix $V$ as $v_{12}^T b = 0$.\n",
    "\n",
    "The terms \\( v_{11} \\) and \\( v_{22} \\) are derived from the **equal norm constraint**:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^\\top \\mathbf{B} \\mathbf{h}_1 = \\mathbf{h}_2^\\top \\mathbf{B} \\mathbf{h}_2\n",
    "$$\n",
    "\n",
    "Expanding these gives:\n",
    "\n",
    "$$\n",
    "v_{11} =\n",
    "\\begin{bmatrix}\n",
    "h_{11}^2 \\\\\n",
    "2 h_{11} h_{21} \\\\\n",
    "h_{21}^2 \\\\\n",
    "2 h_{11} h_{31} \\\\\n",
    "2 h_{21} h_{31} \\\\\n",
    "h_{31}^2\n",
    "\\end{bmatrix}\n",
    "v_{22} =\n",
    "\\begin{bmatrix}\n",
    "h_{12}^2 \\\\\n",
    "2 h_{12} h_{22} \\\\\n",
    "h_{22}^2 \\\\\n",
    "2 h_{12} h_{32} \\\\\n",
    "2 h_{22} h_{32} \\\\\n",
    "h_{32}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which provides another **row** in the constraint matrix $V$ as $(v_{11} - v_{22})^\\top \\mathbf{b} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyTiSTghTZDD"
   },
   "source": [
    "We solve for $V \\mathbf{b} = 0$ using **SVD**. The solution is obtained as the **singular vector corresponding to the smallest singular value**.\n",
    "\n",
    "Finally we e extract the intrinsic parameters:\n",
    "\n",
    "- $v_0 = \\frac{B_{01} B_{02} - B_{00} B_{12}}{B_{00} B_{11} - B_{01}^2}$\n",
    "- $\\lambda = B_{22} - \\frac{B_{02}^2 + v_0 (B_{01} B_{02} - B_{00} B_{12})}{B_{00}}$\n",
    "- $\\alpha = \\sqrt{\\lambda / B_{00}}$ (focal length in $x$)\n",
    "- $\\beta = \\sqrt{\\lambda B_{00} / (B_{00} B_{11} - B_{01}^2)}$ (focal length in $y$)\n",
    "- $\\gamma = -B_{01} \\alpha^2 \\beta / \\lambda$ (skew factor)\n",
    "- $u_0 = \\gamma v_0 / \\beta - B_{02} \\alpha^2 / \\lambda$ (principal point)\n",
    "\n",
    "The intrinsic matrix is then:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} =\n",
    "\\begin{bmatrix}\n",
    "\\alpha & \\gamma & u_0 \\\\\n",
    "0 & \\beta & v_0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: There may be degenerate cases, which is why we check the condition number when solving `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1738471290898,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "fNCOCKD_Pq9L"
   },
   "outputs": [],
   "source": [
    "# Calibrate intrinsics\n",
    "\n",
    "\n",
    "def calibrate_intrinsics(homographies, debug=False):\n",
    "    # Number of homographies\n",
    "    n = len(homographies)\n",
    "    assert n >= 2, \"At least two homographies are required for calibration\"\n",
    "\n",
    "    # Construct the constraint matrix V\n",
    "    V = []\n",
    "    for H in homographies:\n",
    "        h1, h2 = H[:, 0], H[:, 1]\n",
    "\n",
    "        # Compute the constraint vectors\n",
    "        v_12 = np.array(\n",
    "            [\n",
    "                h1[0] * h2[0],\n",
    "                h1[0] * h2[1] + h1[1] * h2[0],\n",
    "                h1[1] * h2[1],\n",
    "                h1[2] * h2[0] + h1[0] * h2[2],\n",
    "                h1[2] * h2[1] + h1[1] * h2[2],\n",
    "                h1[2] * h2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        v_11 = np.array(\n",
    "            [\n",
    "                h1[0] ** 2,\n",
    "                2 * h1[0] * h1[1],\n",
    "                h1[1] ** 2,\n",
    "                2 * h1[0] * h1[2],\n",
    "                2 * h1[1] * h1[2],\n",
    "                h1[2] ** 2,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        v_22 = np.array(\n",
    "            [\n",
    "                h2[0] ** 2,\n",
    "                2 * h2[0] * h2[1],\n",
    "                h2[1] ** 2,\n",
    "                2 * h2[0] * h2[2],\n",
    "                2 * h2[1] * h2[2],\n",
    "                h2[2] ** 2,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        V.append(v_12)\n",
    "        V.append(v_11 - v_22)\n",
    "\n",
    "    V = np.vstack(V)\n",
    "\n",
    "    # Solve for the null space of V to get b (which defines B)\n",
    "    _, S, Vt = np.linalg.svd(V)\n",
    "    if debug:\n",
    "        rank_V = np.linalg.matrix_rank(V)\n",
    "        print(f\"Rank of V: {rank_V}\")\n",
    "        print(\"Singular values of V:\", S)\n",
    "        print(\"Condition number of V:\", S[0] / S[-1])\n",
    "\n",
    "    b = Vt[-1]\n",
    "\n",
    "    # Construct the B matrix\n",
    "    B00, B01, B11, B02, B12, B22 = b\n",
    "    B = np.array([[B00, B01, B02], [B01, B11, B12], [B02, B12, B22]])\n",
    "\n",
    "    # Compute intrinsic parameters\n",
    "    v0 = (B01 * B02 - B00 * B12) / (B00 * B11 - B01**2)\n",
    "    lambda_ = B22 - (B02**2 + v0 * (B01 * B02 - B00 * B12)) / B00\n",
    "    alpha = np.sqrt(lambda_ / B00)\n",
    "    beta = np.sqrt(lambda_ * B00 / (B00 * B11 - B01**2))\n",
    "    gamma = -B01 * alpha**2 * beta / lambda_\n",
    "    u0 = gamma * v0 / beta - B02 * alpha**2 / lambda_\n",
    "\n",
    "    # Intrinsic matrix K\n",
    "    K = np.array([[alpha, gamma, u0], [0, beta, v0], [0, 0, 1]])\n",
    "    # Intrinsic matrix K\n",
    "    K = np.array([[alpha, gamma, u0], [0, beta, v0], [0, 0, 1]])\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AN7u3gxhVjxL"
   },
   "source": [
    "### Camera Extrinsic Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViJdsYXaVlC8"
   },
   "source": [
    "Given $\\mathbf{K}$ and a homography $\\mathbf{H}$, the extrinsic parameters (rotation $\\mathbf{R}$ and translation $\\mathbf{t}$) are computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{K}^{-1} \\mathbf{H}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{r}_1 = \\frac{\\mathbf{M}_{:,1}}{\\lambda}$\n",
    "- $\\mathbf{r}_2 = \\frac{\\mathbf{M}_{:,2}}{\\lambda}$\n",
    "- $\\mathbf{r}_3 = \\mathbf{r}_1 \\times \\mathbf{r}_2$ (ensuring orthogonality)\n",
    "- $\\mathbf{t} = \\frac{\\mathbf{M}_{:,3}}{\\lambda}$\n",
    "\n",
    "The **orthonormalization** step ensures $\\mathbf{R}$ is a valid rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1738471290898,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "uo7zS_GkVwa7"
   },
   "outputs": [],
   "source": [
    "# Compute extrinsic parameters\n",
    "\n",
    "\n",
    "def compute_extrinsics(H, K):\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    M = K_inv @ H\n",
    "    r1 = M[:, 0]\n",
    "    r2 = M[:, 1]\n",
    "    t = M[:, 2]\n",
    "    # r1 and r2 should be unit vectors\n",
    "    lambda_ = (np.linalg.norm(r1) + np.linalg.norm(r2)) / 2\n",
    "    r1 = r1 / lambda_\n",
    "    r2 = r2 / lambda_\n",
    "    r3 = np.cross(r1, r2)\n",
    "    R = np.column_stack((r1, r2, r3))\n",
    "    U, _, Vt = np.linalg.svd(R)\n",
    "    R_ortho = U @ Vt\n",
    "    if np.linalg.det(R_ortho) < 0:\n",
    "        Vt[-1, :] *= -1  # Flip the last row of Vt\n",
    "        R_ortho = U @ Vt\n",
    "    t = t / lambda_\n",
    "    return np.linalg.inv(pt.transform_from(R, t))\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV4zy53_WB2l"
   },
   "source": [
    "### Running the Camera Calibration Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWQew5NjAB_P"
   },
   "source": [
    "Using the simulated world points and camera poses, we generate image observations:\n",
    "\n",
    "1. **Project world points into the image plane** using $\\mathbf{K}$ and camera poses $\\mathbf{T}_{WC}$.\n",
    "2. **Compute homographies** from world-image correspondences.\n",
    "3. **Estimate camera intrinsics** using multiple homographies.\n",
    "4. **Recover camera extrinsics** for each view.\n",
    "\n",
    "To validate the calibration:\n",
    "\n",
    "- **Intrinsic matrix** $\\mathbf{K}_{\\text{tested}}$ should match the ground truth $\\mathbf{K}$.\n",
    "- **Estimated camera poses** $\\mathbf{T}_{WC}^{\\text{tested}}$ should align with the expected ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1738471290898,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "lPJcEKzqphIN",
    "outputId": "c34f45e6-4530-413e-c443-9ee4cea74e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of V: 5\n",
      "Singular values of V: [3.7186e+04 2.8990e+04 3.3617e+03 1.1681e+01 1.6096e+00 1.4826e-16]\n",
      "Condition number of V: 2.508181506987467e+20\n"
     ]
    }
   ],
   "source": [
    "# Camera CalibrationSimulation\n",
    "\n",
    "camera_poses_expected = generate_camera_poses()\n",
    "\n",
    "pixel_obs_list = []\n",
    "for i, T_W_C in enumerate(camera_poses_expected):\n",
    "    # Each column is a point\n",
    "    points_in_C = (np.linalg.inv(T_W_C) @ homogeneous_world_points)[:3, :]\n",
    "    uv = K @ points_in_C\n",
    "    uv_normalized = uv[:2, :] / uv[2, :]\n",
    "    pixel_obs_list.append(uv_normalized)\n",
    "\n",
    "# Compute homographies with normalization\n",
    "homographies = []\n",
    "for T_W_C, pixel_obs in zip(camera_poses_expected, pixel_obs_list):\n",
    "    h = estimate_homography(world_points, pixel_points=pixel_obs)\n",
    "\n",
    "    # Expected homography from intrinsic & extrinsic parameters\n",
    "    T_C_W = np.linalg.inv(T_W_C)\n",
    "    h_expected = K.dot(T_C_W[:3, [0, 1, 3]])\n",
    "    h_expected /= h_expected[2, 2]\n",
    "\n",
    "    # Check if estimated homography is close to expected\n",
    "    assert np.allclose(h, h_expected), f\"{h=}, {h_expected=}\"\n",
    "\n",
    "    homographies.append(h)\n",
    "\n",
    "# Compute intrinsic parameters\n",
    "K_tested = calibrate_intrinsics(homographies=homographies, debug=True)\n",
    "assert np.allclose(K, K_tested), f\"{K=}, {K_tested=}\"\n",
    "\n",
    "\n",
    "# Camera poses are expressed in world frame.\n",
    "camera_poses_tested = []\n",
    "for T_W_C_expected, H in zip(camera_poses_expected, homographies):\n",
    "    T_W_C_tested = compute_extrinsics(H=H, K=K_tested)\n",
    "    camera_poses_tested.append(T_W_C_tested)\n",
    "    assert np.allclose(T_W_C_expected, T_W_C_tested)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLNMLwXOWpsa"
   },
   "source": [
    "## Hand-Eye Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBx4nu0-W0Op"
   },
   "source": [
    "### Hand-Eye Calibration using \\( AX = XB \\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afpuajyOkJPB"
   },
   "source": [
    "I primarily follow `Robot Sensor Calibration: Solving AX = XB on the Euclidean Group`, except that I use SVD to solve for rotation, similar to the approach in point cloud alignment.\n",
    "\n",
    "Hand-eye calibration is the process of estimating the transformation between a **camera** and a **robot end-effector** when the camera is rigidly mounted on the robot. This calibration helps in accurately relating observations from the camera to the robot's movement.\n",
    "\n",
    "In this section, we implement the **Park & Martin method** to solve the classic **hand-eye calibration equation**:\n",
    "\n",
    "$$\n",
    "AX = XB\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A_i$ represents the relative transformation between two **camera** poses $T_{C_iC_j}$\n",
    "- $B_i$ represents the relative transformation between two **end-effector** poses $T_{E_iE_j}$.\n",
    "- $X$ is the unknown transformation from the **camera frame** to the **end-effector frame** $T_{CE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mrug9IEn8Fu"
   },
   "source": [
    "### Estimating Rotation $\\mathbf{R_{CE}}$ and $\\mathbf{t_{CE}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FmqwsmzimZ3"
   },
   "source": [
    "\n",
    "Extracting the rotation component from $T_{C_i C_j} X = X T_{E_i E_j}$,we obtain:\n",
    "$$\n",
    "R_{C_i C_j} R_{CE} = R_{CE} R_{E_i E_j}\n",
    "$$\n",
    "\n",
    "Taking the **logarithm map** of the rotation part:\n",
    "\n",
    "$$\n",
    "\\log( R_{C_i C_j} ) = R \\log( R_{E_i E_j} ) R^{-1}\n",
    "$$\n",
    "\n",
    "Since the **Lie algebra** of rotations follows:\n",
    "\n",
    "$$\n",
    "\\log( R_{C_i C_j} ) = [\\alpha]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log( R_{E_i E_j} ) = [\\beta]\n",
    "$$\n",
    "\n",
    "We obtain the relationship:\n",
    "\n",
    "$$\n",
    "[\\alpha] = R [\\beta] R^{-1}\n",
    "$$\n",
    "\n",
    "Using vector form:\n",
    "\n",
    "$$\n",
    "\\alpha = R \\beta\n",
    "$$\n",
    "\n",
    "Given multiple transformation pairs $\\alpha_i, \\beta_i$, we solve for $R$ by minimizing:\n",
    "\n",
    "$$\n",
    "\\sum \\| \\alpha_i - R \\beta_i \\|^2\n",
    "$$\n",
    "\n",
    "which is solved using **SVD decomposition**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX0H9L37oKHn"
   },
   "source": [
    "\n",
    "Once $R_{CE}$ is known, the equation for translation becomes:\n",
    "\n",
    "$$\n",
    "t_{C_i C_j} = R t_{E_i E_j} + t\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "(I - R) t_{E_i E_j} = t_{C_i C_j} - t\n",
    "$$\n",
    "\n",
    "This forms an **overdetermined linear system**, which is solved using **least squares regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1738471293580,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "__-VX6rWphIN"
   },
   "outputs": [],
   "source": [
    "# calibrate hand eye\n",
    "\n",
    "from scipy.linalg import logm\n",
    "\n",
    "\n",
    "def logm_vector(T):\n",
    "    R = T[:3, :3]\n",
    "    log_R = logm(R)\n",
    "    return np.array([log_R[2, 1], log_R[0, 2], log_R[1, 0]])\n",
    "\n",
    "\n",
    "def calibrate_hand_eye(T_Ci_Cj_list, T_Ei_Ej_list):\n",
    "    assert len(T_Ci_Cj_list) > 1\n",
    "    n_data = len(T_Ci_Cj_list)\n",
    "    M = np.zeros((3, 3))\n",
    "    C = np.zeros((3 * n_data, 3))\n",
    "    d = np.zeros((3 * n_data, 1))\n",
    "\n",
    "    # This is from the example in the paper\n",
    "    if n_data == 2:\n",
    "        alpha = logm_vector(T_Ci_Cj_list[0])\n",
    "        beta = logm_vector(T_Ei_Ej_list[0])\n",
    "        alpha2 = logm_vector(T_Ci_Cj_list[1])\n",
    "        beta2 = logm_vector(T_Ei_Ej_list[1])\n",
    "        alpha3 = np.cross(alpha, alpha2)\n",
    "        beta3 = np.cross(beta, beta2)\n",
    "        A_ = np.vstack([alpha, alpha2, alpha3]).T\n",
    "        B_ = np.vstack([beta, beta2, beta3]).T\n",
    "        R = A_.dot(np.linalg.inv(B_))\n",
    "    else:\n",
    "        for A, B in zip(T_Ci_Cj_list, T_Ei_Ej_list):\n",
    "            alpha = logm_vector(A)\n",
    "            beta = logm_vector(B)\n",
    "            M += beta.reshape(3, 1) @ alpha.reshape(1, 3)\n",
    "        # I am ssing SVD instead of the way described in the paper.\n",
    "        # R = scipy.linalg.sqrtm(np.linalg.inv(M.T @ M)) @ M.T\n",
    "        U, _, Vt = np.linalg.svd(M)\n",
    "        R = Vt.T @ U.T\n",
    "\n",
    "    for i in range(n_data):\n",
    "        Ra = T_Ci_Cj_list[i][0:3, 0:3]\n",
    "        ta = T_Ci_Cj_list[i][0:3, 3]\n",
    "        tb = T_Ei_Ej_list[i][0:3, 3]\n",
    "\n",
    "        C[3 * i : 3 * i + 3, :] = np.eye(3) - Ra\n",
    "        d[3 * i : 3 * i + 3, 0] = ta - R @ tb\n",
    "\n",
    "    t = np.linalg.inv(C.T @ C) @ C.T @ d\n",
    "    return pt.transform_from(R, t.reshape(3))\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp7fUHCxoreD"
   },
   "source": [
    "### Run Hand-Eye Calibration Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfHreBnrozil"
   },
   "source": [
    "To validate the **hand-eye calibration** method, we generate a simulation where a **camera** is rigidly attached to a **robot end-effector**. The simulation involves:\n",
    "\n",
    "1. **Generating ground-truth transformations** for both the camera and end-effector.\n",
    "2. **Computing end-effector poses** based on the expected transformations.\n",
    "3. **Introducing noise** to simulate real-world conditions.\n",
    "4. **Extracting transformation pairs** to solve for hand-eye calibration.\n",
    "5. **Testing the estimated transformation** against the ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1738471295848,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "_dwLhUt7phIO"
   },
   "outputs": [],
   "source": [
    "# Run Hand-eye calibration Simulation\n",
    "\n",
    "T_E_C_expected = pt.transform_from(\n",
    "    R=pr.matrix_from_axis_angle([0, 1, 0, np.radians(88)]),\n",
    "    p=np.array([0.1, 0.05, 0.05]),\n",
    ")\n",
    "T_C_E_expected = np.linalg.inv(T_E_C_expected)\n",
    "T_W_B_expected = pt.transform_from(\n",
    "    R=pr.matrix_from_axis_angle([0, 0, 1, np.radians(90)]), p=np.array([10, 10, 0.05])\n",
    ")\n",
    "T_B_W_expected = np.linalg.inv(T_W_B_expected)\n",
    "\n",
    "\n",
    "def generate_end_effector_poses(\n",
    "    camera_poses_expected, noise_factor=0.0, error_low_bound_m=3, error_high_bound_m=7\n",
    "):\n",
    "    random.seed(1024)\n",
    "    np.random.seed(1024)\n",
    "    errors_expected = np.random.uniform(\n",
    "        error_low_bound_m * noise_factor,\n",
    "        error_high_bound_m * noise_factor,\n",
    "        (len(camera_poses_expected), 3),\n",
    "    )\n",
    "\n",
    "    # These are in Base frame\n",
    "    end_effector_poses = []\n",
    "    end_effector_poses_noisy = []\n",
    "    for i, T_W_C in enumerate(camera_poses_expected):\n",
    "        T_B_E = T_B_W_expected @ T_W_C @ T_C_E_expected\n",
    "        T_B_E_noisy = copy.deepcopy(T_B_E)\n",
    "        T_B_E_noisy[0:3, -1] += errors_expected[i, :]\n",
    "        end_effector_poses.append(T_B_E)\n",
    "        end_effector_poses_noisy.append(T_B_E_noisy)\n",
    "\n",
    "    return (\n",
    "        end_effector_poses,\n",
    "        end_effector_poses_noisy,\n",
    "        [np.array(error) for error in errors_expected],\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_AB_pairs(camera_poses, end_effector_poses, n_pairs=2, T_C_E_expected=None):\n",
    "    random.seed(1024)\n",
    "    np.random.seed(1024)\n",
    "    # pick n random pairs\n",
    "    T_Ci_Cj_list = []\n",
    "    T_Ei_Ej_list = []\n",
    "    indics = range(len(end_effector_poses))\n",
    "    selected_pairs = []\n",
    "\n",
    "    while len(selected_pairs) < n_pairs:\n",
    "        pair = random.sample(indics, 2)\n",
    "        sorted_pair = tuple(sorted(pair))  # Ensure pairs are ordered\n",
    "        if sorted_pair not in selected_pairs:\n",
    "            selected_pairs.append(sorted_pair)\n",
    "\n",
    "    for pair in selected_pairs:\n",
    "        T_W_Ci = camera_poses[pair[0]]\n",
    "        T_W_Cj = camera_poses[pair[1]]\n",
    "        T_Ci_Cj = np.linalg.inv(T_W_Ci) @ T_W_Cj\n",
    "        T_B_Ei = end_effector_poses[pair[0]]\n",
    "        T_B_Ej = end_effector_poses[pair[1]]\n",
    "        T_Ei_Ej = np.linalg.inv(T_B_Ei) @ T_B_Ej\n",
    "        T_Ci_Cj_list.append(T_Ci_Cj)\n",
    "        T_Ei_Ej_list.append(T_Ei_Ej)\n",
    "        if T_C_E_expected is not None:\n",
    "            # Whe we have perfect inputs, we can check AX = XB\n",
    "            assert np.allclose(T_Ci_Cj @ T_C_E_expected, T_C_E_expected @ T_Ei_Ej)\n",
    "    return T_Ci_Cj_list, T_Ei_Ej_list\n",
    "\n",
    "\n",
    "noise_factor = 1e-3\n",
    "n_pairs = 2\n",
    "end_effector_poses, end_effector_poses_noisy, errors_expected = (\n",
    "    generate_end_effector_poses(camera_poses_expected, noise_factor=noise_factor)\n",
    ")\n",
    "\n",
    "T_Ci_Cj_list, T_Ei_Ej_list = generate_AB_pairs(\n",
    "    camera_poses_expected,\n",
    "    end_effector_poses,\n",
    "    n_pairs=n_pairs,\n",
    "    T_C_E_expected=T_C_E_expected,\n",
    ")\n",
    "T_C_E_tested = calibrate_hand_eye(T_Ci_Cj_list, T_Ei_Ej_list)\n",
    "assert np.allclose(T_C_E_tested, T_C_E_expected)\n",
    "# AX = XB -> T_Ci_Cj @ T_C_E_expected = T_C_E_expected @ T_Ei_Ej\n",
    "# We can also have T_Wi_Wj @ T_W_B_expected = T_W_B_expected @ T_Bi_Bj\n",
    "T_Wi_Wj_list, T_Bi_Bj_list = generate_AB_pairs(\n",
    "    [np.linalg.inv(T_W_C) for T_W_C in camera_poses_expected],\n",
    "    [np.linalg.inv(T_B_E) for T_B_E in end_effector_poses],\n",
    "    n_pairs=2,\n",
    "    T_C_E_expected=T_W_B_expected,\n",
    ")\n",
    "T_W_B_tested = calibrate_hand_eye(T_Wi_Wj_list, T_Bi_Bj_list)\n",
    "assert np.allclose(T_W_B_tested, T_W_B_expected)\n",
    "\n",
    "\n",
    "T_Ci_Cj_list, T_Ei_Ej_list = generate_AB_pairs(\n",
    "    camera_poses_tested, end_effector_poses, n_pairs=n_pairs\n",
    ")\n",
    "T_C_E_tested = calibrate_hand_eye(T_Ci_Cj_list, T_Ei_Ej_list)\n",
    "# This seems that our camera_poses_tested is quite good...\n",
    "assert np.allclose(T_C_E_tested, T_C_E_expected)\n",
    "\n",
    "# Finally, try use noisy estimation\n",
    "T_Ci_Cj_list, T_Ei_Ej_list = generate_AB_pairs(\n",
    "    camera_poses_tested, end_effector_poses_noisy, n_pairs=n_pairs\n",
    ")\n",
    "T_C_E_tested = calibrate_hand_eye(T_Ci_Cj_list, T_Ei_Ej_list)\n",
    "\n",
    "T_Wi_Wj_list, T_Bi_Bj_list = generate_AB_pairs(\n",
    "    [np.linalg.inv(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    [np.linalg.inv(T_B_E) for T_B_E in end_effector_poses],\n",
    "    n_pairs=2,\n",
    "    T_C_E_expected=T_W_B_expected,\n",
    ")\n",
    "# or T_W_B_tested = camera_poses_tested[0] @ T_C_E_tested @ np.linalg.inv(end_effector_poses_noisy[0])\n",
    "T_W_B_tested = calibrate_hand_eye(T_Wi_Wj_list, T_Bi_Bj_list)\n",
    "T_B_W_tested = np.linalg.inv(T_W_B_tested)\n",
    "\n",
    "# assert np.allclose(T_C_E_tested, T_C_E_expected)\n",
    "# assert np.allclose(T_W_B_tested, T_W_B_expected)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWmmWQ2Ioo4p"
   },
   "source": [
    "## Factor Graph based Hand-Eye Calibration Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT9Sp-zqtwdF"
   },
   "source": [
    "Several degenerate cases can arise in the previous processes, such as insufficient views to estimate $H$ or $K$ and inadequate movement to determine $T_{CE}$. Additionally, real-world measurements are imperfect, with errors in pixel observations, robot arm end-effector poses, and other factors.\n",
    "\n",
    "When performing hand-eye calibration, it is ideal to capture a diverse range of calibration board views and execute various robot arm movements. Given the limited field of view of a single camera with a single calibration board, using multiple boards placed throughout the room can improve accuracy.\n",
    "\n",
    "To address these issues after we have enought data, we formulate the problem as an optimization task to estimate the variables of interest.\n",
    "\n",
    "Note: In this tutorial, we will not simulate the optimization of camera intrinsics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV-Xro8BtxHS"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1738471312218,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "_FjkEbf_phIP"
   },
   "outputs": [],
   "source": [
    "# Import symforce\n",
    "\n",
    "import symforce\n",
    "\n",
    "symforce.set_epsilon_to_symbol()\n",
    "\n",
    "import numpy as np\n",
    "from symforce.values import Values\n",
    "import symforce.symbolic as sf\n",
    "\n",
    "from symforce.opt.factor import Factor\n",
    "from symforce.opt.optimizer import Optimizer\n",
    "\n",
    "import sym\n",
    "\n",
    "\n",
    "def symforce_from_projection_matrix(T):\n",
    "    return sym.Pose3(R=sym.Rot3.from_rotation_matrix(T[0:3, 0:3]), t=T[0:3, 3])\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEo7lONt7_w"
   },
   "source": [
    "### Define Residual Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMpQ7xx0v6Nz"
   },
   "source": [
    "The function `reprojection_residual_from_camera` computes the reprojection error of a landmark observed by a camera. It assumes that the camera pose is being optimized in the world frame.\n",
    "\n",
    "- $T_{W_C}^{-1} \\cdot W_t^{landmark}$ transforms the landmark position into the camera frame.\n",
    "- $K \\cdot (...)$ projects the transformed point into image coordinates.\n",
    "- The residual is computed as the difference between the observed pixel location\n",
    "- $uv_{obs}$ and the projected landmark position.\n",
    "\n",
    "\n",
    "To enforce consistency in transformations, the function `pose_loop_residual` ensures that the transformation chain forms a valid loop $T_{BB} = T_{BW} \\cdot T_{WC} \\cdot T_{CE} \\cdot (T_{BE_n} \\cdot T_{E_nE})^{-1}$.\n",
    "\n",
    "NOTE: We might also want to have a prior constraint on error correction term $T_{E_nE}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1738471313938,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "AGRPMw6dphIP"
   },
   "outputs": [],
   "source": [
    "# Residual functions\n",
    "\n",
    "\n",
    "# This is purely optimizing camera poses in world frame\n",
    "def reprojection_residual_from_camera(\n",
    "    T_W_C: sf.Pose3,\n",
    "    W_t_landmark: sf.V3,\n",
    "    uv_obs: sf.V2,\n",
    "    K: sf.Matrix33,\n",
    "    epsilon: sf.Scalar,\n",
    ") -> sf.V2:\n",
    "    uv = K * (T_W_C.inverse() * W_t_landmark)\n",
    "    # TODO: This is not safe, should use epsilon. And obs could go out of view.\n",
    "    return sf.V2(uv_obs[0] - uv[0] / uv[2], uv_obs[1] - uv[1] / uv[2])\n",
    "\n",
    "\n",
    "# This is not using camera poses at all. If we combine this with above,\n",
    "# It seems that we can have the pose loop constraint as well:\n",
    "# T_B_W * T_W_C * T_C_E * T_B_E_correction.inverse()\n",
    "def reprojection_residual_from_arm(\n",
    "    T_B_W: sf.Pose3,\n",
    "    T_B_E: sf.Pose3,\n",
    "    T_B_E_position_error: sf.V3,\n",
    "    T_C_E: sf.Pose3,\n",
    "    W_t_landmark: sf.V3,\n",
    "    uv_obs: sf.V2,\n",
    "    K: sf.Matrix33,\n",
    "    epsilon: sf.Scalar,\n",
    ") -> sf.V2:\n",
    "    T_B_E_correction = sf.Pose3(\n",
    "        R=T_B_E.rotation(), t=T_B_E_position_error - T_B_E.position()\n",
    "    )\n",
    "    T_C_B = T_C_E * T_B_E_correction.inverse()\n",
    "    uv = K * (T_C_B * T_B_W * W_t_landmark)\n",
    "    # TODO: This is not safe, should use epsilon\n",
    "    return sf.V2(uv_obs[0] - uv[0] / uv[2], uv_obs[1] - uv[1] / uv[2])\n",
    "\n",
    "\n",
    "def pose_loop_residual(\n",
    "    T_W_C: sf.Pose3,\n",
    "    T_B_W: sf.Pose3,\n",
    "    T_B_E: sf.Pose3,\n",
    "    T_B_E_position_error: sf.V3,\n",
    "    T_C_E: sf.Pose3,\n",
    "    epsilon: sf.Scalar,\n",
    ") -> sf.V2:\n",
    "    T_B_E_correction = sf.Pose3(\n",
    "        R=T_B_E.rotation(), t=T_B_E.position() - T_B_E_position_error\n",
    "    )\n",
    "    T_B_B = T_B_W * T_W_C * T_C_E * T_B_E_correction.inverse()\n",
    "    return T_B_B.to_tangent(epsilon=epsilon)\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5rplX4jxL3b"
   },
   "source": [
    "### Generate Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1738471317052,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "Nz5qXv4kphIQ"
   },
   "outputs": [],
   "source": [
    "# Generate different factors\n",
    "\n",
    "\n",
    "def generate_factors(\n",
    "    n_camera_poses,\n",
    "    n_world_points,\n",
    "    use_reprojection_residual_from_camera=False,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=False,\n",
    "):\n",
    "    factors = []\n",
    "    if use_reprojection_residual_from_camera:\n",
    "        for i in range(n_camera_poses):\n",
    "            for j in range(n_world_points):\n",
    "                factors.append(\n",
    "                    Factor(\n",
    "                        residual=reprojection_residual_from_camera,\n",
    "                        keys=[\n",
    "                            f\"T_W_Cs[{i}]\",\n",
    "                            f\"W_t_landmarks[{j}]\",\n",
    "                            f\"uv_obs[{i}][{j}]\",\n",
    "                            \"K\",\n",
    "                            \"epsilon\",\n",
    "                        ],\n",
    "                    )\n",
    "                )\n",
    "    if use_reprojection_residual_from_arm:\n",
    "        for i in range(n_camera_poses):\n",
    "            for j in range(n_world_points):\n",
    "                factors.append(\n",
    "                    Factor(\n",
    "                        residual=use_reprojection_residual_from_arm,\n",
    "                        keys=[\n",
    "                            \"T_B_W\",\n",
    "                            f\"T_B_Es[{i}]\",\n",
    "                            f\"T_B_E_position_errors[{i}]\",\n",
    "                            \"T_C_E\",\n",
    "                            f\"W_t_landmarks[{j}]\",\n",
    "                            f\"uv_obs[{i}][{j}]\",\n",
    "                            \"K\",\n",
    "                            \"epsilon\",\n",
    "                        ],\n",
    "                    )\n",
    "                )\n",
    "    if use_pose_residual:\n",
    "        for i in range(n_camera_poses):\n",
    "            factors.append(\n",
    "                Factor(\n",
    "                    residual=pose_loop_residual,\n",
    "                    keys=[\n",
    "                        f\"T_W_Cs[{i}]\",\n",
    "                        \"T_B_W\",\n",
    "                        f\"T_B_Es[{i}]\",\n",
    "                        f\"T_B_E_position_errors[{i}]\",\n",
    "                        \"T_C_E\",\n",
    "                        \"epsilon\",\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "    assert len(factors) > 0\n",
    "    return factors\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRH69aO-xTld"
   },
   "source": [
    "### Optimize Camera Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdcSLJtmxT_r"
   },
   "source": [
    "The pose estimation from camera calibration appears to be quite accurate, as the residual reduction is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1738471321836,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "2pwhsJb0phIQ",
    "outputId": "cb819108-ed2e-47bd-beea-e725ae4301d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.iterations)=3\n",
      "Initial error: 3.4163439793428014e-21\n",
      "Final error: 5.505113158739144e-25\n"
     ]
    }
   ],
   "source": [
    "# Only optimize camera poses\n",
    "\n",
    "uv_obs = []\n",
    "for pixel_obs in pixel_obs_list:\n",
    "    obs_per_camera = []\n",
    "    for obs in pixel_obs.T:\n",
    "        obs_per_camera.append(sf.V2(obs[0], obs[1]))\n",
    "    uv_obs.append(obs_per_camera)\n",
    "\n",
    "W_t_landmarks = []\n",
    "for world_point in world_points:\n",
    "    W_t_landmarks.append(sf.V3(world_point[0], world_point[1], world_point[2]))\n",
    "\n",
    "inputs = Values(\n",
    "    T_W_Cs=[symforce_from_projection_matrix(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    W_t_landmarks=W_t_landmarks,\n",
    "    uv_obs=uv_obs,\n",
    "    K=K,\n",
    "    epsilon=sf.numeric_epsilon,\n",
    ")\n",
    "\n",
    "factors = generate_factors(\n",
    "    n_camera_poses=len(camera_poses_expected),\n",
    "    n_world_points=len(world_points),\n",
    "    use_reprojection_residual_from_camera=True,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=False,\n",
    ")\n",
    "optimized_keys = []\n",
    "optimized_keys.extend([f\"T_W_Cs[{i}]\" for i in range(len(camera_poses_tested))])\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    # So that we save more information babout each iteration, to visualize later:\n",
    "    debug_stats=True,\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(inputs)\n",
    "assert result.status == Optimizer.Status.SUCCESS\n",
    "print(f\"{len(result.iterations)=}\")\n",
    "\n",
    "initial_linearization = optimizer.linearize(result.initial_values)\n",
    "print(f\"Initial error: {initial_linearization.error()}\")\n",
    "\n",
    "optimized_linearization = optimizer.linearize(result.optimized_values)\n",
    "print(f\"Final error: {optimized_linearization.error()}\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLLGMXsf1yK_"
   },
   "source": [
    "### Optimize End Effector Pose Corrections and $T_{CE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 973,
     "status": "ok",
     "timestamp": 1738471684331,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "L0cN54KB153n",
    "outputId": "02bb35db-bfde-40bc-80d9-19c9c87be2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.iterations)=11\n",
      "Initial error: 0.005298482748792415\n",
      "Final error: 1.2656542091601458e-13\n",
      "========\n",
      "T_CE(initial)  :  [ 0.0452 -0.0588 -0.0945]\n",
      "T_CE(optimized):  [ 0.0448 -0.0458 -0.1041]\n",
      "T_CE(expected) :  [ 0.0465 -0.05   -0.1017]\n",
      "========\n",
      "expected_errors : [array([0.0056, 0.007 , 0.0051]), array([0.0056, 0.0054, 0.006 ]), array([0.0035, 0.003 , 0.0036]), array([0.0058, 0.0054, 0.0066]), array([0.0048, 0.0043, 0.0067]), array([0.0049, 0.0062, 0.0038]), array([0.0063, 0.0036, 0.0057]), array([0.0032, 0.0061, 0.0059]), array([0.0061, 0.0037, 0.0032]), array([0.0066, 0.0037, 0.0061]), array([0.0041, 0.0056, 0.007 ]), array([0.0041, 0.0047, 0.0061]), array([0.004 , 0.0032, 0.0065]), array([0.004 , 0.0046, 0.0042]), array([0.0069, 0.0053, 0.0041]), array([0.0065, 0.0033, 0.0061]), array([0.0056, 0.0038, 0.006 ]), array([0.0068, 0.006 , 0.0057]), array([0.0054, 0.0038, 0.0047]), array([0.0062, 0.006 , 0.0058]), array([0.0034, 0.0063, 0.005 ]), array([0.0052, 0.005 , 0.0049]), array([0.0034, 0.0065, 0.0037]), array([0.0044, 0.0038, 0.0051]), array([0.0066, 0.0041, 0.0059]), array([0.0061, 0.003 , 0.0064]), array([0.0058, 0.0057, 0.0039]), array([0.0052, 0.0069, 0.0039]), array([0.0062, 0.0034, 0.0058]), array([0.0036, 0.0041, 0.0069]), array([0.0069, 0.0036, 0.0068]), array([0.0064, 0.0069, 0.0044])]\n",
      "optimized_errors: [array([0.004 , 0.007 , 0.0002]), array([0.004 , 0.0046, 0.0012]), array([ 0.0019,  0.0014, -0.001 ]), array([0.0042, 0.0029, 0.0023]), array([0.0056, 0.0028, 0.0018]), array([ 0.0064,  0.0052, -0.001 ]), array([0.0085, 0.003 , 0.0011]), array([0.0062, 0.0059, 0.0017]), array([ 0.0069,  0.0052, -0.0017]), array([0.0066, 0.0055, 0.0013]), array([0.0035, 0.0079, 0.0023]), array([0.0028, 0.0073, 0.0018]), array([0.0024, 0.0032, 0.0016]), array([ 0.0024,  0.0038, -0.0006]), array([ 0.0053,  0.0036, -0.0005]), array([0.0049, 0.0009, 0.0018]), array([0.0039, 0.0038, 0.0011]), array([0.0052, 0.0052, 0.0009]), array([3.7438e-03, 2.0942e-03, 9.9781e-05]), array([0.0046, 0.0036, 0.0016]), array([4.2196e-03, 4.9197e-03, 9.1918e-05]), array([6.7145e-03, 3.9874e-03, 3.2584e-05]), array([ 0.0056,  0.0059, -0.0009]), array([0.0073, 0.0036, 0.0009]), array([0.0074, 0.0055, 0.001 ]), array([0.0062, 0.0049, 0.0016]), array([ 0.0052,  0.008 , -0.0007]), array([ 0.0039,  0.0096, -0.0003]), array([0.0046, 0.0034, 0.0009]), array([0.0019, 0.0033, 0.0021]), array([0.0053, 0.0019, 0.0022]), array([0.0048, 0.0044, 0.0002])]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "# Optimize T_B_E_position_errors\n",
    "\n",
    "uv_obs = []\n",
    "for pixel_obs in pixel_obs_list:\n",
    "    obs_per_camera = []\n",
    "    for obs in pixel_obs.T:\n",
    "        obs_per_camera.append(sf.V2(obs[0], obs[1]))\n",
    "    uv_obs.append(obs_per_camera)\n",
    "\n",
    "W_t_landmarks = []\n",
    "for world_point in world_points:\n",
    "    W_t_landmarks.append(sf.V3(world_point[0], world_point[1], world_point[2]))\n",
    "\n",
    "inputs = Values(\n",
    "    T_W_Cs=[symforce_from_projection_matrix(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    T_B_W=symforce_from_projection_matrix(T_B_W_tested),\n",
    "    T_B_Es=[\n",
    "        symforce_from_projection_matrix(T_B_E) for T_B_E in end_effector_poses_noisy\n",
    "    ],\n",
    "    T_B_E_position_errors=[sf.V3(0, 0, 0)] * len(end_effector_poses_noisy),\n",
    "    T_C_E=symforce_from_projection_matrix(T_C_E_tested),\n",
    "    W_t_landmarks=W_t_landmarks,\n",
    "    uv_obs=uv_obs,\n",
    "    K=K,\n",
    "    epsilon=sf.numeric_epsilon,\n",
    ")\n",
    "\n",
    "# Let's just try to optimize T_C_E, and T_B_E_position_errors\n",
    "\n",
    "factors = generate_factors(\n",
    "    n_camera_poses=len(camera_poses_expected),\n",
    "    n_world_points=len(world_points),\n",
    "    use_reprojection_residual_from_camera=False,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=True,\n",
    ")\n",
    "\n",
    "optimized_keys = []\n",
    "optimized_keys.extend(\n",
    "    [f\"T_B_E_position_errors[{i}]\" for i in range(len(end_effector_poses_noisy))]\n",
    ")\n",
    "optimized_keys.extend([\"T_C_E\"])\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    # So that we save more information babout each iteration, to visualize later:\n",
    "    debug_stats=True,\n",
    "    params=Optimizer.Params(\n",
    "        verbose=False,\n",
    "        iterations=1000,\n",
    "        early_exit_min_reduction=1e-6,\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(inputs)\n",
    "assert result.status == Optimizer.Status.SUCCESS\n",
    "print(f\"{len(result.iterations)=}\")\n",
    "\n",
    "initial_linearization = optimizer.linearize(result.initial_values)\n",
    "print(f\"Initial error: {initial_linearization.error()}\")\n",
    "\n",
    "optimized_linearization = optimizer.linearize(result.optimized_values)\n",
    "print(f\"Final error: {optimized_linearization.error()}\")\n",
    "\n",
    "if True:\n",
    "    print(\"========\")\n",
    "    print(\"T_CE(initial)  : \", result.initial_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(optimized): \", result.optimized_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(expected) : \", T_C_E_expected[:, -1][0:3])\n",
    "    print(\"========\")\n",
    "    np.set_printoptions(precision=4, suppress=False)\n",
    "    print(\"expected_errors :\", [np.array(error) for error in errors_expected])\n",
    "    print(\"optimized_errors:\", result.optimized_values.attr.T_B_E_position_errors)\n",
    "    print(\"========\")\n",
    "\n",
    "previous_result = result\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1r4dBr7xZmq"
   },
   "source": [
    "### Optimize End Effector Pose Corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8D6BpLuxZzB"
   },
   "source": [
    "Now, let's focus on optimizing the end-effector pose correction $T_{E_nE}$. The relative error reduction is significant, even though the absolute system error with the initial value is quite small (we have perfect measurements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1162,
     "status": "ok",
     "timestamp": 1738472017401,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "HaDM-Wco9Dde",
    "outputId": "b803fe10-a196-492a-fef5-d3ecf7afcfc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.iterations)=8\n",
      "Initial error: 0.005298482748792109\n",
      "Final error: 3.1803870859392485e-13\n",
      "========\n",
      "expected_errors : [array([0.0056, 0.007 , 0.0051]), array([0.0056, 0.0054, 0.006 ]), array([0.0035, 0.003 , 0.0036]), array([0.0058, 0.0054, 0.0066]), array([0.0048, 0.0043, 0.0067]), array([0.0049, 0.0062, 0.0038]), array([0.0063, 0.0036, 0.0057]), array([0.0032, 0.0061, 0.0059]), array([0.0061, 0.0037, 0.0032]), array([0.0066, 0.0037, 0.0061]), array([0.0041, 0.0056, 0.007 ]), array([0.0041, 0.0047, 0.0061]), array([0.004 , 0.0032, 0.0065]), array([0.004 , 0.0046, 0.0042]), array([0.0069, 0.0053, 0.0041]), array([0.0065, 0.0033, 0.0061]), array([0.0056, 0.0038, 0.006 ]), array([0.0068, 0.006 , 0.0057]), array([0.0054, 0.0038, 0.0047]), array([0.0062, 0.006 , 0.0058]), array([0.0034, 0.0063, 0.005 ]), array([0.0052, 0.005 , 0.0049]), array([0.0034, 0.0065, 0.0037]), array([0.0044, 0.0038, 0.0051]), array([0.0066, 0.0041, 0.0059]), array([0.0061, 0.003 , 0.0064]), array([0.0058, 0.0057, 0.0039]), array([0.0052, 0.0069, 0.0039]), array([0.0062, 0.0034, 0.0058]), array([0.0036, 0.0041, 0.0069]), array([0.0069, 0.0036, 0.0068]), array([0.0064, 0.0069, 0.0044])]\n",
      "optimized_errors: [array([0.0043, 0.0052, 0.0163]), array([0.0043, 0.0055, 0.0173]), array([0.0023, 0.0051, 0.0147]), array([0.0045, 0.0094, 0.0172]), array([0.007 , 0.0041, 0.0179]), array([0.0054, 0.005 , 0.0151]), array([0.0051, 0.0014, 0.0169]), array([0.0004, 0.003 , 0.0165]), array([0.0052, 0.0058, 0.0144]), array([0.0073, 0.0047, 0.0174]), array([0.0066, 0.0057, 0.0181]), array([0.0082, 0.0038, 0.0167]), array([0.0027, 0.0014, 0.0176]), array([0.0027, 0.0048, 0.0156]), array([0.0056, 0.0074, 0.0152]), array([0.0052, 0.0073, 0.0167]), array([0.0043, 0.002 , 0.0172]), array([0.0055, 0.0062, 0.017 ]), array([0.0041, 0.0059, 0.0158]), array([0.005 , 0.0101, 0.0164]), array([0.0056, 0.0061, 0.0162]), array([0.0057, 0.0038, 0.0162]), array([0.0022, 0.0043, 0.0148]), array([0.0016, 0.0006, 0.0157]), array([0.0057, 0.0061, 0.0171]), array([0.0069, 0.0041, 0.0177]), array([0.0083, 0.0058, 0.015 ]), array([0.0094, 0.006 , 0.0145]), array([0.0049, 0.0016, 0.017 ]), array([0.0023, 0.0043, 0.0182]), array([0.0056, 0.0057, 0.018 ]), array([0.0051, 0.0109, 0.015 ])]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "# Optimize T_B_E_position_errors\n",
    "\n",
    "uv_obs = []\n",
    "for pixel_obs in pixel_obs_list:\n",
    "    obs_per_camera = []\n",
    "    for obs in pixel_obs.T:\n",
    "        obs_per_camera.append(sf.V2(obs[0], obs[1]))\n",
    "    uv_obs.append(obs_per_camera)\n",
    "\n",
    "W_t_landmarks = []\n",
    "for world_point in world_points:\n",
    "    W_t_landmarks.append(sf.V3(world_point[0], world_point[1], world_point[2]))\n",
    "\n",
    "inputs = Values(\n",
    "    T_W_Cs=[symforce_from_projection_matrix(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    T_B_W=symforce_from_projection_matrix(T_B_W_tested),\n",
    "    T_B_Es=[\n",
    "        symforce_from_projection_matrix(T_B_E) for T_B_E in end_effector_poses_noisy\n",
    "    ],\n",
    "    T_B_E_position_errors=[sf.V3(0, 0, 0)] * len(end_effector_poses_noisy),\n",
    "    T_C_E=symforce_from_projection_matrix(T_C_E_tested),\n",
    "    W_t_landmarks=W_t_landmarks,\n",
    "    uv_obs=uv_obs,\n",
    "    K=K,\n",
    "    epsilon=sf.numeric_epsilon,\n",
    ")\n",
    "\n",
    "factors = generate_factors(\n",
    "    n_camera_poses=len(camera_poses_expected),\n",
    "    n_world_points=len(world_points),\n",
    "    use_reprojection_residual_from_camera=False,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=True,\n",
    ")\n",
    "\n",
    "optimized_keys = []\n",
    "optimized_keys.extend(\n",
    "    [f\"T_B_E_position_errors[{i}]\" for i in range(len(end_effector_poses_noisy))]\n",
    ")\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    # So that we save more information babout each iteration, to visualize later:\n",
    "    debug_stats=True,\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(inputs)\n",
    "assert result.status == Optimizer.Status.SUCCESS\n",
    "print(f\"{len(result.iterations)=}\")\n",
    "\n",
    "initial_linearization = optimizer.linearize(result.initial_values)\n",
    "print(f\"Initial error: {initial_linearization.error()}\")\n",
    "\n",
    "optimized_linearization = optimizer.linearize(result.optimized_values)\n",
    "print(f\"Final error: {optimized_linearization.error()}\")\n",
    "\n",
    "if True:\n",
    "    print(\"========\")\n",
    "    np.set_printoptions(precision=4, suppress=False)\n",
    "    print(\"expected_errors :\", [np.array(error) for error in errors_expected])\n",
    "    print(\"optimized_errors:\", result.optimized_values.attr.T_B_E_position_errors)\n",
    "    print(\"========\")\n",
    "\n",
    "previous_result = result\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEB8bGpIxsCb"
   },
   "source": [
    "### Optimize End Effector Pose Corrections and $T_{CE}$ again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDihY5tt1Pk2"
   },
   "source": [
    "After getting end-effector pose correction $T_{E_nE}$, I try to optimize both $T_{E_nE}$ and $T_{CE}$. Interestingly, there is no movement of $T_{CE}$ but just $T_{E_nE}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1738472028044,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "3V8OknZXC3Oh",
    "outputId": "813848cc-0956-4c7e-ec29-5ab423075da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.iterations)=10\n",
      "Initial error: 3.180387086041939e-13\n",
      "Final error: 1.2656542081530388e-13\n",
      "========\n",
      "T_CE(initial)  :  [ 0.0452 -0.0588 -0.0945]\n",
      "T_CE(optimized):  [ 0.0452 -0.0588 -0.0945]\n",
      "T_CE(expected) :  [ 0.0465 -0.05   -0.1017]\n",
      "========\n",
      "expected_errors : [array([0.0056, 0.007 , 0.0051]), array([0.0056, 0.0054, 0.006 ]), array([0.0035, 0.003 , 0.0036]), array([0.0058, 0.0054, 0.0066]), array([0.0048, 0.0043, 0.0067]), array([0.0049, 0.0062, 0.0038]), array([0.0063, 0.0036, 0.0057]), array([0.0032, 0.0061, 0.0059]), array([0.0061, 0.0037, 0.0032]), array([0.0066, 0.0037, 0.0061]), array([0.0041, 0.0056, 0.007 ]), array([0.0041, 0.0047, 0.0061]), array([0.004 , 0.0032, 0.0065]), array([0.004 , 0.0046, 0.0042]), array([0.0069, 0.0053, 0.0041]), array([0.0065, 0.0033, 0.0061]), array([0.0056, 0.0038, 0.006 ]), array([0.0068, 0.006 , 0.0057]), array([0.0054, 0.0038, 0.0047]), array([0.0062, 0.006 , 0.0058]), array([0.0034, 0.0063, 0.005 ]), array([0.0052, 0.005 , 0.0049]), array([0.0034, 0.0065, 0.0037]), array([0.0044, 0.0038, 0.0051]), array([0.0066, 0.0041, 0.0059]), array([0.0061, 0.003 , 0.0064]), array([0.0058, 0.0057, 0.0039]), array([0.0052, 0.0069, 0.0039]), array([0.0062, 0.0034, 0.0058]), array([0.0036, 0.0041, 0.0069]), array([0.0069, 0.0036, 0.0068]), array([0.0064, 0.0069, 0.0044])]\n",
      "optimized_errors: [array([0.0043, 0.0052, 0.0163]), array([0.0043, 0.0055, 0.0173]), array([0.0023, 0.0051, 0.0147]), array([0.0045, 0.0094, 0.0172]), array([0.007 , 0.0041, 0.0179]), array([0.0054, 0.005 , 0.0151]), array([0.0051, 0.0014, 0.0169]), array([0.0004, 0.003 , 0.0165]), array([0.0052, 0.0058, 0.0144]), array([0.0073, 0.0047, 0.0174]), array([0.0066, 0.0057, 0.0181]), array([0.0082, 0.0038, 0.0167]), array([0.0027, 0.0014, 0.0176]), array([0.0027, 0.0048, 0.0156]), array([0.0056, 0.0074, 0.0152]), array([0.0052, 0.0073, 0.0167]), array([0.0043, 0.002 , 0.0172]), array([0.0055, 0.0062, 0.017 ]), array([0.0041, 0.0059, 0.0158]), array([0.005 , 0.0101, 0.0164]), array([0.0056, 0.0061, 0.0162]), array([0.0057, 0.0038, 0.0162]), array([0.0022, 0.0043, 0.0148]), array([0.0016, 0.0006, 0.0157]), array([0.0057, 0.0061, 0.0171]), array([0.0069, 0.0041, 0.0177]), array([0.0083, 0.0058, 0.015 ]), array([0.0094, 0.006 , 0.0145]), array([0.0049, 0.0016, 0.017 ]), array([0.0023, 0.0043, 0.0182]), array([0.0056, 0.0057, 0.018 ]), array([0.0051, 0.0109, 0.015 ])]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "# Let's just try to optimize T_C_E and T_B_E_position_errors\n",
    "\n",
    "T_B_E_position_errors = [\n",
    "    sf.V3(error[0], error[1], error[2])\n",
    "    for error in previous_result.optimized_values.attr.T_B_E_position_errors\n",
    "]\n",
    "\n",
    "if False:\n",
    "    T_B_E_position_errors = [\n",
    "        sf.V3(error[0], error[1], error[2]) for error in errors_expected\n",
    "    ]\n",
    "\n",
    "inputs = Values(\n",
    "    T_W_Cs=[symforce_from_projection_matrix(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    T_B_W=symforce_from_projection_matrix(T_B_W_tested),\n",
    "    T_B_Es=[\n",
    "        symforce_from_projection_matrix(T_B_E) for T_B_E in end_effector_poses_noisy\n",
    "    ],\n",
    "    T_B_E_position_errors=T_B_E_position_errors,\n",
    "    T_C_E=symforce_from_projection_matrix(T_C_E_tested),\n",
    "    W_t_landmarks=W_t_landmarks,\n",
    "    uv_obs=uv_obs,\n",
    "    K=K,\n",
    "    epsilon=sf.numeric_epsilon,\n",
    ")\n",
    "\n",
    "factors = generate_factors(\n",
    "    n_camera_poses=len(camera_poses_expected),\n",
    "    n_world_points=len(world_points),\n",
    "    use_reprojection_residual_from_camera=False,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=True,\n",
    ")\n",
    "\n",
    "optimized_keys = []\n",
    "optimized_keys.extend(\n",
    "    [f\"T_B_E_position_errors[{i}]\" for i in range(len(end_effector_poses_noisy))]\n",
    ")\n",
    "optimized_keys.extend([\"T_C_E\"])\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    # So that we save more information babout each iteration, to visualize later:\n",
    "    debug_stats=True,\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(inputs)\n",
    "assert result.status == Optimizer.Status.SUCCESS\n",
    "print(f\"{len(result.iterations)=}\")\n",
    "\n",
    "initial_linearization = optimizer.linearize(result.initial_values)\n",
    "print(f\"Initial error: {initial_linearization.error()}\")\n",
    "\n",
    "optimized_linearization = optimizer.linearize(result.optimized_values)\n",
    "print(f\"Final error: {optimized_linearization.error()}\")\n",
    "\n",
    "if True:\n",
    "    print(\"========\")\n",
    "    print(\"T_CE(initial)  : \", result.initial_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(optimized): \", result.optimized_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(expected) : \", T_C_E_expected[:, -1][0:3])\n",
    "    print(\"========\")\n",
    "    np.set_printoptions(precision=4, suppress=False)\n",
    "    print(\"expected_errors :\", [np.array(error) for error in errors_expected])\n",
    "    print(\"optimized_errors:\", result.optimized_values.attr.T_B_E_position_errors)\n",
    "    print(\"========\")\n",
    "\n",
    "previous_result = result\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1738472039230,
     "user": {
      "displayName": "Xipeng Wang",
      "userId": "10769873257184456020"
     },
     "user_tz": 300
    },
    "id": "f4AgdAQQ-nf1",
    "outputId": "07afd20c-3349-45e6-c990-ebb4594e2f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.iterations)=11\n",
      "Initial error: 0.0020829916578865106\n",
      "Final error: 1.2656542085960234e-13\n",
      "========\n",
      "T_CE(initial)  :  [ 0.0452 -0.0588 -0.0945]\n",
      "T_CE(optimized):  [ 0.0464 -0.0503 -0.1015]\n",
      "T_CE(expected) :  [ 0.0465 -0.05   -0.1017]\n",
      "========\n",
      "expected_errors : [array([0.0056, 0.007 , 0.0051]), array([0.0056, 0.0054, 0.006 ]), array([0.0035, 0.003 , 0.0036]), array([0.0058, 0.0054, 0.0066]), array([0.0048, 0.0043, 0.0067]), array([0.0049, 0.0062, 0.0038]), array([0.0063, 0.0036, 0.0057]), array([0.0032, 0.0061, 0.0059]), array([0.0061, 0.0037, 0.0032]), array([0.0066, 0.0037, 0.0061]), array([0.0041, 0.0056, 0.007 ]), array([0.0041, 0.0047, 0.0061]), array([0.004 , 0.0032, 0.0065]), array([0.004 , 0.0046, 0.0042]), array([0.0069, 0.0053, 0.0041]), array([0.0065, 0.0033, 0.0061]), array([0.0056, 0.0038, 0.006 ]), array([0.0068, 0.006 , 0.0057]), array([0.0054, 0.0038, 0.0047]), array([0.0062, 0.006 , 0.0058]), array([0.0034, 0.0063, 0.005 ]), array([0.0052, 0.005 , 0.0049]), array([0.0034, 0.0065, 0.0037]), array([0.0044, 0.0038, 0.0051]), array([0.0066, 0.0041, 0.0059]), array([0.0061, 0.003 , 0.0064]), array([0.0058, 0.0057, 0.0039]), array([0.0052, 0.0069, 0.0039]), array([0.0062, 0.0034, 0.0058]), array([0.0036, 0.0041, 0.0069]), array([0.0069, 0.0036, 0.0068]), array([0.0064, 0.0069, 0.0044])]\n",
      "optimized_errors: [array([0.0056, 0.0069, 0.0054]), array([0.0056, 0.0054, 0.0064]), array([0.0035, 0.0031, 0.0039]), array([0.0058, 0.0055, 0.0069]), array([0.0048, 0.0043, 0.007 ]), array([0.0049, 0.0062, 0.0041]), array([0.0062, 0.0035, 0.0061]), array([0.0031, 0.006 , 0.0063]), array([0.0061, 0.0038, 0.0035]), array([0.0066, 0.0037, 0.0064]), array([0.0042, 0.0056, 0.0073]), array([0.0042, 0.0046, 0.0064]), array([0.004 , 0.0032, 0.0068]), array([0.004 , 0.0046, 0.0046]), array([0.0068, 0.0054, 0.0044]), array([0.0064, 0.0034, 0.0064]), array([0.0055, 0.0037, 0.0063]), array([0.0068, 0.006 , 0.006 ]), array([0.0053, 0.0038, 0.005 ]), array([0.0062, 0.0062, 0.0062]), array([0.0035, 0.0063, 0.0053]), array([0.0052, 0.0049, 0.0052]), array([0.0033, 0.0064, 0.004 ]), array([0.0043, 0.0037, 0.0054]), array([0.0066, 0.0042, 0.0062]), array([0.0061, 0.0031, 0.0067]), array([0.0059, 0.0057, 0.0042]), array([0.0054, 0.0069, 0.0043]), array([0.0062, 0.0034, 0.0061]), array([0.0035, 0.0042, 0.0072]), array([0.0068, 0.0037, 0.0072]), array([0.0064, 0.007 , 0.0048])]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "# Let's just try to optimize T_C_E and T_B_E_position_errors (Cheat on initial guesses)\n",
    "\n",
    "T_B_E_position_errors = [\n",
    "    sf.V3(error[0], error[1], error[2])\n",
    "    for error in previous_result.optimized_values.attr.T_B_E_position_errors\n",
    "]\n",
    "\n",
    "if True:  # Cheating... :-)\n",
    "    T_B_E_position_errors = [\n",
    "        sf.V3(error[0], error[1], error[2]) for error in errors_expected\n",
    "    ]\n",
    "\n",
    "inputs = Values(\n",
    "    T_W_Cs=[symforce_from_projection_matrix(T_W_C) for T_W_C in camera_poses_tested],\n",
    "    T_B_W=symforce_from_projection_matrix(T_B_W_tested),\n",
    "    T_B_Es=[\n",
    "        symforce_from_projection_matrix(T_B_E) for T_B_E in end_effector_poses_noisy\n",
    "    ],\n",
    "    T_B_E_position_errors=T_B_E_position_errors,\n",
    "    T_C_E=symforce_from_projection_matrix(T_C_E_tested),\n",
    "    W_t_landmarks=W_t_landmarks,\n",
    "    uv_obs=uv_obs,\n",
    "    K=K,\n",
    "    epsilon=sf.numeric_epsilon,\n",
    ")\n",
    "\n",
    "\n",
    "factors = generate_factors(\n",
    "    n_camera_poses=len(camera_poses_expected),\n",
    "    n_world_points=len(world_points),\n",
    "    use_reprojection_residual_from_camera=False,\n",
    "    use_reprojection_residual_from_arm=False,\n",
    "    use_pose_residual=True,\n",
    ")\n",
    "\n",
    "optimized_keys = []\n",
    "optimized_keys.extend(\n",
    "    [f\"T_B_E_position_errors[{i}]\" for i in range(len(end_effector_poses_noisy))]\n",
    ")\n",
    "optimized_keys.extend([\"T_C_E\"])\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    factors=factors,\n",
    "    optimized_keys=optimized_keys,\n",
    "    # So that we save more information babout each iteration, to visualize later:\n",
    "    debug_stats=True,\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(inputs)\n",
    "assert result.status == Optimizer.Status.SUCCESS\n",
    "print(f\"{len(result.iterations)=}\")\n",
    "\n",
    "initial_linearization = optimizer.linearize(result.initial_values)\n",
    "print(f\"Initial error: {initial_linearization.error()}\")\n",
    "\n",
    "optimized_linearization = optimizer.linearize(result.optimized_values)\n",
    "print(f\"Final error: {optimized_linearization.error()}\")\n",
    "\n",
    "if True:\n",
    "    print(\"========\")\n",
    "    print(\"T_CE(initial)  : \", result.initial_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(optimized): \", result.optimized_values.attr.T_C_E.position())\n",
    "    print(\"T_CE(expected) : \", T_C_E_expected[:, -1][0:3])\n",
    "    print(\"========\")\n",
    "    np.set_printoptions(precision=4, suppress=False)\n",
    "    print(\"expected_errors :\", [np.array(error) for error in errors_expected])\n",
    "    print(\"optimized_errors:\", result.optimized_values.attr.T_B_E_position_errors)\n",
    "    print(\"========\")\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BIQD-7Q2ExL5",
    "SrcLA-0pE4mB",
    "foRXM0d8FxI1",
    "qpSFozUtGHKJ",
    "79nPjw07GmST",
    "wBrtbAEnI5uj",
    "dIyGKcICJKc7",
    "VpOyxqczPc-1",
    "AN7u3gxhVjxL",
    "tBx4nu0-W0Op",
    "1mrug9IEn8Fu",
    "Hp7fUHCxoreD",
    "vV-Xro8BtxHS"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
