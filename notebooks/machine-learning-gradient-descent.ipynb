{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-mobile",
   "metadata": {},
   "source": [
    "# Machine Learning Notes - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-sudan",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[Pythoch optimizer]({% post_url 2021-05-04-machine-learning-pytorch %}#pytorch-section-optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-rotation",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-saturday",
   "metadata": {},
   "source": [
    "$$w := w - \\text{learning_rate} \\times \\frac{\\partial{L}}{\\partial{w}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liable-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "    '''\n",
    "    Performs vanilla stochastic gradient descent.\n",
    "    config format:\n",
    "      - learning_rate: Scalar learning rate.\n",
    "    '''\n",
    "    \n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "\n",
    "    w -= config['learning_rate'] * dw\n",
    "    return w, config\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-windsor",
   "metadata": {},
   "source": [
    "## SGD + Momentum\n",
    "\n",
    "$$\n",
    "v = \\mu \\times v - \\text{learning_rate} \\times \\frac{\\partial{L}}{\\partial{w}} \\\\\n",
    "w := w + v\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baking-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(w, dw, config=None):\n",
    "    '''\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "    config format:\n",
    "      - learning_rate: Scalar learning rate.\n",
    "      - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "        Setting momentum = 0 reduces to sgd.\n",
    "      - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "        moving average of the gradients.\n",
    "    '''\n",
    "    \n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    config.setdefault('momentum', 0.9)\n",
    "    v = config.get('velocity', torch.zeros_like(w))\n",
    "    next_w = None\n",
    "    \n",
    "    v = config['momentum']*v-config['learning_rate']*dw\n",
    "    next_w = w + v\n",
    "    \n",
    "    config['velocity'] = v\n",
    "    return next_w, config\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-authority",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "RMSProp is an update rule that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "$$\n",
    "\\text{cache} = \\text{decay_rate} \\times \\text{cache} + (1 - \\text{decay_rate}) \\times \\frac{\\partial{L}}{\\partial{w}} .* \\frac{\\partial{L}}{\\partial{w}}\\\\\n",
    "w := w - \\frac{\\text{learning_rate} \\times \\frac{\\partial{L}}{\\partial{w}}}{(\\sqrt{\\text{cache}} + \\epsilon)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unable-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(w, dw, config=None):\n",
    "  '''\n",
    "  Uses the RMSProp update rule, which uses a moving average of squared\n",
    "  gradient values to set adaptive per-parameter learning rates.\n",
    "  config format:\n",
    "  - learning_rate: Scalar learning rate.\n",
    "  - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n",
    "    gradient cache.\n",
    "  - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "  - cache: Moving average of second moments of gradients.\n",
    "  '''\n",
    "\n",
    "  if config is None: config = {}\n",
    "  config.setdefault('learning_rate', 1e-2)\n",
    "  config.setdefault('decay_rate', 0.99)\n",
    "  config.setdefault('epsilon', 1e-8)\n",
    "  config.setdefault('cache', torch.zeros_like(w))\n",
    "\n",
    "  next_w = None\n",
    "  config['cache'] = config['decay_rate']*config['cache'] + (1- config['decay_rate']) * dw * dw\n",
    "  next_w = w-config['learning_rate']*dw / (torch.sqrt(config['cache']) + config['epsilon'])\n",
    "\n",
    "  return next_w, config\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-broad",
   "metadata": {},
   "source": [
    "## Adam\n",
    "Adam extends RMSprop with a first-order gradient cache similar to momentum, and a bias correction mechanism to prevent large steps at the start of optimization. \n",
    "$$\n",
    "m = \\beta_1 \\times m + (1-\\beta_1) \\times \\frac{\\partial{L}}{\\partial{w}} \\\\\n",
    "v = \\beta_2 \\times v + (1-\\beta_2)\\times(\\frac{\\partial{L}}{\\partial{w}}.*\\frac{\\partial{L}}{\\partial{w}}) \\\\\n",
    "x += - \\frac{\\text{learning_rate} \\times m}{\\sqrt{v} + eps} \\\\\n",
    "$$\n",
    "\n",
    "The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors $m$,$v$ are both initialized and therefore biased at zero, before they fully “warm up”. With the bias correction mechanism, the update looks as follows:\n",
    "$$\n",
    "m = \\beta_1 \\times m + (1-\\beta_1)\\times \\frac{\\partial{L}}{\\partial{w}} \\\\\n",
    "m_t = \\frac{m}{1-\\beta_1^t} \\\\ \n",
    "v = \\beta_2 \\times v + (1-\\beta_2) \\times (\\frac{\\partial{L}}{\\partial{w}}.* \\frac{\\partial{L}}{\\partial{w}}) \\\\\n",
    "v_t = \\frac{v}{1-\\beta_2^t} \\\\\n",
    "w := w - \\frac{\\text{learning_rate} \\times m_t}{\\sqrt{v_t} + \\epsilon} \\\\ \n",
    "$$\n",
    "**Where $t$ is your iteration counter going from $1$ to infinity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "former-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(w, dw, config=None):\n",
    "  '''\n",
    "  Uses the Adam update rule, which incorporates moving averages of both the\n",
    "  gradient and its square and a bias correction term.\n",
    "  config format:\n",
    "  - learning_rate: Scalar learning rate.\n",
    "  - beta1: Decay rate for moving average of first moment of gradient.\n",
    "  - beta2: Decay rate for moving average of second moment of gradient.\n",
    "  - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "  - m: Moving average of gradient.\n",
    "  - v: Moving average of squared gradient.\n",
    "  - t: Iteration number.\n",
    "  '''\n",
    "\n",
    "  if config is None: config = {}\n",
    "  config.setdefault('learning_rate', 1e-3)\n",
    "  config.setdefault('beta1', 0.9)\n",
    "  config.setdefault('beta2', 0.999)\n",
    "  config.setdefault('epsilon', 1e-8)\n",
    "  config.setdefault('m', torch.zeros_like(w))\n",
    "  config.setdefault('v', torch.zeros_like(w))\n",
    "  config.setdefault('t', 0)\n",
    "\n",
    "  next_w = None\n",
    "  config['t'] = config['t'] + 1\n",
    "  b1 = config['beta1']\n",
    "  b2 = config['beta2']\n",
    "  t = config['t']\n",
    "  m = b1*config['m']+(1-b1)*dw\n",
    "  mt = m / (1-b1**t)\n",
    "  v = b2*config['v']+(1-b2)*dw*dw\n",
    "  vt = v / (1-b2**t)\n",
    "  next_w = w - config['learning_rate']*mt / (torch.sqrt(vt) + config['epsilon'])\n",
    "  config['m'] = m\n",
    "  config['v'] = v\n",
    "  return next_w, config\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
