{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "skilled-costume",
   "metadata": {},
   "source": [
    "# Machine Learning Notes - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-credits",
   "metadata": {},
   "source": [
    "## A List of Related Posts\n",
    "1. [Pytorch]({% post_url 2021-05-04-machine-learning-pytorch %})\n",
    "2. [Loss function]({% post_url 2021-05-07-machine-learning-loss %})\n",
    "3. [Backpropagation(this)]({% post_url 2021-05-07-machine-learning-backpropagation %})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-cancellation",
   "metadata": {},
   "source": [
    "## Backpropagation Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-nickel",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\n",
    "y = f_1(x) \\\\\n",
    "L = f_2(y)\n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\frac{\\partial{f_1(x)}}{\\partial{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "periodic-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: 6\n",
      "Pytorch autograd tensor([6.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([5], requires_grad=True, dtype=torch.float64)\n",
    "y = 2 * x\n",
    "L = 3 * y\n",
    "dL = 1\n",
    "dy = 3 * dL\n",
    "dx = 2 * dy\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-hydrogen",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix} = f(\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "f_{1}(x_1, x_2, x_3) \\\\\n",
    "f_{2}(x_1, x_2, x_3)\n",
    "\\end{bmatrix}\\\\\n",
    "L = g(\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix}) \\\\ \n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\begin{bmatrix} \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_1}} \\\\ \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_2}} \\\\ \n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = J_x^y \\frac{\\partial{L}}{\\partial{y}} =\n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_1}}, \\frac{\\partial{f_2(x)}}{\\partial{x_1}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_2}}, \\frac{\\partial{f_2(x)}}{\\partial{x_2}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_3}}, \\frac{\\partial{f_2(x)}}{\\partial{x_3}}\n",
    "\\end{bmatrix} \\frac{\\partial{L}}{\\partial{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pacific-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: tensor([1, 3, 2])\n",
      "Pytorch autograd tensor([1., 3., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gradients with respect to a vector\n",
    "\n",
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64)\n",
    "y = torch.zeros(2)\n",
    "y[0] = x[0] + 2*x[1]\n",
    "y[1] = x[1] + 2*x[2]\n",
    "L = y.sum()\n",
    "dL = 1\n",
    "dy = torch.tensor([1, 1])\n",
    "dx = torch.tensor([[1, 0],[2, 1], [0, 2]]).matmul(dy)\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-registrar",
   "metadata": {},
   "source": [
    "## Local Gradient\n",
    "\n",
    "... ---> $\\textbf{x}$ ---> $f(\\textbf{x})$ ---> $\\textbf{y}$ ---> ... ---> L\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{\\textbf{x}}} = J^{\\textbf{y}}_\\textbf{x}\\frac{\\partial{L}}{\\partial{\\textbf{y}}} \n",
    "$$\n",
    "$J^{\\textbf{y}}_\\textbf{x}$ is a Jacobian matrix, and it has the same number of rows as $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-corner",
   "metadata": {},
   "source": [
    "## Common Computation Blocks - Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-weight",
   "metadata": {},
   "source": [
    "### Vector dot product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "z = x^Ty\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{z}}y \\\\\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{L}}{\\partial{z}}x\n",
    "$$\n",
    "Where $[x]_{nx1}, [y]_{nx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sapphire-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz: 1.0\n",
      "Pytorch autograd dx: tensor([3., 2., 1.], dtype=torch.float64)\n",
      "Pytorch autograd dy: tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector dot product\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64)\n",
    "y = torch.tensor([3, 2, 1], requires_grad=True, dtype=torch.float64)\n",
    "z = x.T.matmul(y)\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz: {z.grad}')\n",
    "print(f'Pytorch autograd dx: {x.grad}')\n",
    "print(f'Pytorch autograd dy: {y.grad}')\n",
    "\n",
    "dx = z.grad * y\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "dy = z.grad * x\n",
    "assert(torch.all(torch.isclose(dy, y.grad)))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-gabriel",
   "metadata": {},
   "source": [
    "### Vector product\n",
    "Forward pass:\n",
    "$$\n",
    "Z = xy\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{Z}}y^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{y}} = x^T\\frac{\\partial{L}}{\\partial{Z}}\n",
    "$$\n",
    "Where $[x]_{mx1}, [y]_{1xn}, [Z]_{mxn}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "informed-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[5.],\n",
      "        [5.],\n",
      "        [5.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[6., 6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64).reshape(3,1)\n",
    "y = torch.tensor([3, 2], requires_grad=True, dtype=torch.float64).reshape(1,2)\n",
    "z = x.matmul(y)\n",
    "x.retain_grad()\n",
    "y.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {y.grad}')\n",
    "\n",
    "dx = z.grad.matmul(y.T)\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "dy = x.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dy, y.grad)))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-convenience",
   "metadata": {},
   "source": [
    "<a id ='backpropagation-section-vector-matrix-product'></a>\n",
    "### Vector & Matrix product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "Z = Wx\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = W^T\\frac{\\partial{L}}{\\partial{Z}} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{Z}}x^T\n",
    "$$\n",
    "Where $[x]_{Dx1}, [W]_{CxD}, [z]_{Cx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abroad-elder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1.],\n",
      "        [1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[3.],\n",
      "        [3.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[1., 2.],\n",
      "        [1., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2], requires_grad=True, dtype=torch.float64).reshape(2,1)\n",
    "w = torch.tensor([[1, 2],[2, 1]], requires_grad=True, dtype=torch.float64).reshape(2,2)\n",
    "z = w.matmul(x)\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {w.grad}')\n",
    "\n",
    "dx = w.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dx, x.grad)))\n",
    "\n",
    "dw = z.grad.matmul(x.T)\n",
    "assert(torch.allclose(dw, w.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-muscle",
   "metadata": {},
   "source": [
    "### Matrix & Matrix product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "Z = XW\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{X}} = \\frac{\\partial{L}}{\\partial{Z}}W^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} = X^T\\frac{\\partial{L}}{\\partial{Z}}\n",
    "$$\n",
    "Where $[x]_{NxD}, [W]_{DxC}, [z]_{NxC}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exposed-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([[1, 2, 3], [3, 2, 1]], requires_grad=True, dtype=torch.float64).reshape(2,3)\n",
    "w = torch.tensor([[1, 2, 3],[2, 1, 3], [3, 1, 2]], requires_grad=True, dtype=torch.float64).reshape(3,3)\n",
    "z = x.matmul(w)\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {w.grad}')\n",
    "\n",
    "dw = x.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dw, w.grad)))\n",
    "\n",
    "dx = z.grad.matmul(w.T)\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-kingston",
   "metadata": {},
   "source": [
    "<a id='backpropagation-section-softmax'></a>\n",
    "### Softmax \n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "p_i = \\frac{e^{s_i}}{\\sum_i{e^{s_i}}} \\\\\n",
    "t = \\sum_i{e^{s_i}}\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{s}} = J^p_s\\frac{\\partial{L}}{\\partial{p}} \\\\\n",
    "\\begin{cases}\n",
    "-p_ip_j &\\text{$i \\neq j$}\\\\\n",
    "p_i(1-p_i) &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $[s]_{Cx1}, [p]_{Cx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "scheduled-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dp:\n",
      " tensor([[-3.7183],\n",
      "        [-1.3679]], dtype=torch.float64)\n",
      "Pytorch autograd ds:\n",
      " tensor([[-0.4621],\n",
      "        [ 0.4621]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "s = torch.tensor([[1, 2]], requires_grad=True, dtype=torch.float64).reshape(2,1)\n",
    "s_exp = torch.exp(s)\n",
    "t = torch.sum(s_exp)\n",
    "p = s_exp / t\n",
    "s.retain_grad()\n",
    "p.retain_grad()\n",
    "# log likelihood\n",
    "L = -torch.sum(torch.log(p))\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dp:\\n {p.grad}')\n",
    "print(f'Pytorch autograd ds:\\n {s.grad}')\n",
    "\n",
    "J = torch.zeros(2, 2, dtype=p.dtype)\n",
    "J[0, 0] = p[0]*(1-p[0]) # i = 0, i = 0\n",
    "J[1, 0] = -p[0]*p[1]    # i = 0, j = 1\n",
    "J[0, 1] = -p[1]*p[0]     # i = 1, j = 0\n",
    "J[1, 1] = p[1]*(1-p[1]) # i = 0, i = 1\n",
    "ds = J.matmul(p.grad)\n",
    "\n",
    "assert(torch.allclose(ds, s.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-flush",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-ozone",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The idea of computation graph is to divide the whole computation into small blocks. During the forward pass, we calculate the block outputs and record the outputs for calculating gradients in backward pass. In the backward pass, we calculate the gradients. This is exactly the same idea of the chain rule, but make it more clear when apply the chain rule. \n",
    "\n",
    "For example, if we have a computation chain is like $ L = (2x + 3y)z$, where $x=1$, $y =2$, $z=3$, we can break it down as:\n",
    "$$\n",
    "a = 2x \\\\\n",
    "b = 3y \\\\\n",
    "c = a+b \\\\\n",
    "L = cz\n",
    "$$\n",
    "\n",
    "In the forward pass, we get:\n",
    "$$\n",
    "a = 2x = 2 \\\\\n",
    "b = 3y = 6\\\\\n",
    "c = a+b = 8\\\\\n",
    "L = cz = 24\n",
    "$$\n",
    "\n",
    "In the backward pass, we get:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{L}} = 1 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{z}} = \\frac{\\partial{L}}{\\partial{L}}\\frac{\\partial{L}}{\\partial{z}} = 1\\times8=8\\\\\n",
    "\\frac{\\partial{L}}{\\partial{c}} = \\frac{\\partial{L}}{\\partial{L}}\\frac{\\partial{L}}{\\partial{c}} = 1\\times3=3\\\\\n",
    "\\frac{\\partial{L}}{\\partial{a}} = \\frac{\\partial{c}}{\\partial{a}}\\frac{\\partial{L}}{\\partial{c}} = 1\\times3=3\\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}} = \\frac{\\partial{c}}{\\partial{b}}\\frac{\\partial{L}}{\\partial{c}} = 1\\times3=3\\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{a}}{\\partial{x}}\\frac{\\partial{L}}{\\partial{a}} = 2\\times3=6\\\\\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{b}}{\\partial{y}}\\frac{\\partial{L}}{\\partial{b}} = 3\\times3=9\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-surge",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "The mathematical expression of Cross Entropy Loss is described [here]({% post_url 2021-05-07-machine-learning-loss %}#loss-section-cross-entropy-loss).\n",
    "\n",
    "A computation graph is shown as below that $[]$ are the outputs of block operation $()$. \n",
    "\n",
    "$[W]_{C \\times D}$ -\\\n",
    "\n",
    "         \\\n",
    "        \n",
    "         (Wx) -> [s]_{Cx1} -> (softmax(s)) -> [p]_{Cx1} -> (cross_entropy_loss(p, label=c)) -> [loss]_{1x1}\n",
    "         \n",
    "         / \n",
    "        \n",
    "$[x]_{D \\times 1}$ -/\n",
    "\n",
    "Consider a single case that $[W]_{3x2}$, $[x]_{2x1}$ and correct label index is $0$.\n",
    "\n",
    "For the forward pass, we can get:\n",
    "$$\n",
    "[s]_{3x1} = Wx \\\\\n",
    "[p]_{3x1} = softmax(s) \\\\\n",
    "p_s = p[0] = \\begin{bmatrix}1, 0, 0\\end{bmatrix}p\\\\\n",
    "L = -log(p_s)\n",
    "$$\n",
    "\n",
    "For the backward pass, we can derive the intermediate gradients one by one.\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{L}} = 1 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{p_s}} = -\\frac{1}{p_s} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{p}} = -\\frac{1}{p_s} \\begin{bmatrix}1 \\\\ 0\\\\ 0\\end{bmatrix} = \\begin{bmatrix}-\\frac{1}{p_s} \\\\ 0\\\\ 0\\end{bmatrix} \n",
    "$$\n",
    "Based on [Softmax](#backpropagation-section-softmax):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{s}} = J^p_s\\frac{\\partial{L}}{\\partial{p}} = \n",
    "\\begin{bmatrix} \n",
    "&p_0(1-p_0), &-p_0p_1, &-p_0p_2 \\\\\n",
    "&-p_1p_0, &p_1(1-p_1), &-p_1p_2\\\\\n",
    "&-p_2p_0, &-p_2p_1, &p_2(1-p_2)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}-\\frac{1}{p_s} \\\\ 0\\\\ 0\\end{bmatrix} = \n",
    "\\begin{bmatrix}p_0-1 \\\\ p_1\\\\ p_2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Based on [Vector & Matrix product](#backpropagation-section-vector-matrix-product):\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{s}}x^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = W^T\\frac{\\partial{L}}{\\partial{s}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "coupled-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabily distribution\n",
      " tensor([0.0391, 0.6168, 0.3442], grad_fn=<DivBackward0>)\n",
      "\n",
      "Cross entropy loss is 3.2429\n",
      "\n",
      "Gradients of s: \n",
      " tensor([-0.9609,  0.6168,  0.3442])\n",
      "\n",
      "Gradients of W: \n",
      " tensor([[-0.9903,  0.2445],\n",
      "        [ 0.6356, -0.1569],\n",
      "        [ 0.3547, -0.0876]], grad_fn=<MmBackward>)\n",
      "\n",
      "Gradients of x: \n",
      " tensor([ 2.1871, -0.7751], grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "W = torch.randn(3, 2, requires_grad = True)\n",
    "x = torch.randn(2, requires_grad = True)\n",
    "c = torch.tensor(0)\n",
    "\n",
    "# Forward\n",
    "s = torch.matmul(W, x)\n",
    "s.retain_grad()\n",
    "# Numeric trick to prevent overflow of using exp().\n",
    "exp_s = torch.exp(s - torch.max(s)) \n",
    "p = exp_s / torch.sum(exp_s)\n",
    "loss = -torch.log(p[c])\n",
    "print(f'\\nProbabily distribution\\n {p}')\n",
    "print(f'\\nCross entropy loss is {loss:.4f}')\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "ds = torch.tensor([p[0]-1, p[1], p[2]])\n",
    "dW = ds.view(-1,1).matmul(x.view(1,-1))\n",
    "dx = W.T.matmul(ds)\n",
    "print(f'\\nGradients of s: \\n {ds}')\n",
    "assert(torch.allclose(ds, s.grad))\n",
    "print(f'\\nGradients of W: \\n {dW}')\n",
    "assert(torch.allclose(dW, W.grad))\n",
    "print(f'\\nGradients of x: \\n {dx}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-dylan",
   "metadata": {},
   "source": [
    "### Hinge  Loss\n",
    "\n",
    "The mathematical expression of Cross Entropy Loss is described [here]({% post_url 2021-05-07-machine-learning-loss %}#loss-section-hinge-loss).\n",
    "\n",
    "A computation graph is shown as below that $[]$ are the outputs of block operation $()$. \n",
    "\n",
    "$[W]_{C \\times D}$ -\\\n",
    "\n",
    "         \\\n",
    "        \n",
    "         (Wx)->[s]_{Cx1}->(s_i-s_c+1, 0)->[t]->(max(t, 0)) ->[z] ->(sum(z)) ->[loss]_{1x1}\n",
    "         \n",
    "         / \n",
    "        \n",
    "$[x]_{D \\times 1}$ -/\n",
    "\n",
    "Consider a single case that $[W]_{3x2}$, $[x]_{2x1}$ and correct label index is $0$.\n",
    "\n",
    "For the forward pass, we can get:\n",
    "$$\n",
    "[s]_{3x1} = Wx \\\\\n",
    "[t]_{3x1} = \\begin{bmatrix}0 \\\\\n",
    "s_1-s_0+1\\\\\n",
    "s_2-s_0+1\\end{bmatrix} \\\\\n",
    "[z]_{3x1} = \\begin{bmatrix}0 \\\\ \n",
    "m(s_1-s_0+1) \\\\ \n",
    "n(s_2-s_0+1)\n",
    "\\end{bmatrix} \\\\\n",
    "L = z_1 + z_2\n",
    "$$\n",
    "Where $m=0$ if $s_1-s_0+1<=0$, otherwise $m=1$, $n=0$ if $s_2-s_0+1<=0$, otherwise $n=1$.\n",
    "\n",
    "For the backward pass, we can derive the intermediate gradients one by one.\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{L}} = 1 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{z}} = \\begin{bmatrix}0 \\\\ 1\\\\ 1\\end{bmatrix} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{t}} = J^z_t\\frac{\\partial{L}}{\\partial{z}}=  \\begin{bmatrix}\n",
    "&0, &0, &0 \\\\ \n",
    "&0, &m, &0 \\\\\n",
    "&0, &0, &n\n",
    "\\end{bmatrix} \\begin{bmatrix}0 \\\\ 1\\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ m \\\\ n\\end{bmatrix} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{s}} = J^t_s\\frac{\\partial{L}}{\\partial{t}} = \n",
    "\\begin{bmatrix}\n",
    "&0, &-1, &-1 \\\\ \n",
    "&0, &1, &0 \\\\\n",
    "&0, &0, &1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}0 \\\\ m \\\\ n\\end{bmatrix} = \\begin{bmatrix}-(m+n) \\\\ m \\\\ n\\end{bmatrix}\n",
    "$$ \n",
    "Based on [Vector & Matrix product](#backpropagation-section-vector-matrix-product):\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{s}}x^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = W^T\\frac{\\partial{L}}{\\partial{s}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "alternative-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Hinge loss is 9.0000\n",
      "\n",
      "Gradients of s: \n",
      " tensor([-2.,  1.,  1.])\n",
      "\n",
      "Gradients of W: \n",
      " tensor([[-4., -2.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  1.]], grad_fn=<MmBackward>)\n",
      "\n",
      "Gradients of x: \n",
      " tensor([2., 3.], grad_fn=<MvBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "W = torch.tensor([[1., 2.], [3., 4.], [1., 3.]], requires_grad = True)\n",
    "x = torch.tensor([2., 1.],  requires_grad = True)\n",
    "c = torch.tensor(0)\n",
    "\n",
    "# Forward\n",
    "s = torch.matmul(W, x)\n",
    "s.retain_grad()\n",
    "t = s-s[c]+1\n",
    "t[c] = 0\n",
    "z = torch.clamp(t, 0)\n",
    "m = float(t[1] > 0)\n",
    "n = float(t[2] > 0)\n",
    "loss = z[1] + z[2]\n",
    "print(f'\\Hinge loss is {loss:.4f}')\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "ds = torch.tensor([-m-n, m, n])\n",
    "dW = ds.view(-1,1).matmul(x.view(1,-1))\n",
    "dx = W.T.matmul(ds)\n",
    "print(f'\\nGradients of s: \\n {ds}')\n",
    "assert(torch.allclose(ds, s.grad))\n",
    "\n",
    "print(f'\\nGradients of W: \\n {dW}')\n",
    "assert(torch.allclose(dW, W.grad))\n",
    "print(f'\\nGradients of x: \\n {dx}')\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
