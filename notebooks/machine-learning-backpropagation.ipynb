{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "skilled-costume",
   "metadata": {},
   "source": [
    "# Machine Learning Notes - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-credits",
   "metadata": {},
   "source": [
    "## A List of Related Posts\n",
    "1. [Pytorch]({% post_url 2021-05-04-machine-learning-pytorch %})\n",
    "2. [Loss function]({% post_url 2021-05-07-machine-learning-loss %})\n",
    "3. [Backpropagation(this)]({% post_url 2021-05-07-machine-learning-backpropagation %})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-cancellation",
   "metadata": {},
   "source": [
    "## Backpropagation Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-nickel",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\n",
    "y = f_1(x) \\\\\n",
    "L = f_2(y)\n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\frac{\\partial{f_1(x)}}{\\partial{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "periodic-appliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: 6\n",
      "Pytorch autograd tensor([6.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([5], requires_grad=True, dtype=torch.float64)\n",
    "y = 2 * x\n",
    "L = 3 * y\n",
    "dL = 1\n",
    "dy = 3 * dL\n",
    "dx = 2 * dy\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-hydrogen",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix} = f(\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "f_{1}(x_1, x_2, x_3) \\\\\n",
    "f_{2}(x_1, x_2, x_3)\n",
    "\\end{bmatrix}\\\\\n",
    "L = g(\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix}) \\\\ \n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\begin{bmatrix} \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_1}} \\\\ \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_2}} \\\\ \n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = J_x^y \\frac{\\partial{L}}{\\partial{y}} =\n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_1}}, \\frac{\\partial{f_2(x)}}{\\partial{x_1}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_2}}, \\frac{\\partial{f_2(x)}}{\\partial{x_2}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_3}}, \\frac{\\partial{f_2(x)}}{\\partial{x_3}}\n",
    "\\end{bmatrix} \\frac{\\partial{L}}{\\partial{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "pacific-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: tensor([1, 3, 2])\n",
      "Pytorch autograd tensor([1., 3., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gradients with respect to a vector\n",
    "\n",
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64)\n",
    "y = torch.zeros(2)\n",
    "y[0] = x[0] + 2*x[1]\n",
    "y[1] = x[1] + 2*x[2]\n",
    "L = y.sum()\n",
    "dL = 1\n",
    "dy = torch.tensor([1, 1])\n",
    "dx = torch.tensor([[1, 0],[2, 1], [0, 2]]).matmul(dy)\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-registrar",
   "metadata": {},
   "source": [
    "## Local Gradient\n",
    "\n",
    "... ---> $\\textbf{x}$ ---> $f(\\textbf{x})$ ---> $\\textbf{y}$ ---> ... ---> L\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{\\textbf{x}}} = J^{\\textbf{y}}_\\textbf{x}\\frac{\\partial{L}}{\\partial{\\textbf{y}}} \n",
    "$$\n",
    "$J^{\\textbf{y}}_\\textbf{x}$ is a Jacobian matrix, and it has the same number of rows as $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-corner",
   "metadata": {},
   "source": [
    "## Common Computation Blocks - Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-weight",
   "metadata": {},
   "source": [
    "### Vector dot product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "z = x^Ty\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{z}}y \\\\\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{L}}{\\partial{z}}x\n",
    "$$\n",
    "Where $[x]_{nx1}, [y]_{nx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sapphire-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz: 1.0\n",
      "Pytorch autograd dx: tensor([3., 2., 1.], dtype=torch.float64)\n",
      "Pytorch autograd dy: tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector dot product\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64)\n",
    "y = torch.tensor([3, 2, 1], requires_grad=True, dtype=torch.float64)\n",
    "z = x.T.matmul(y)\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz: {z.grad}')\n",
    "print(f'Pytorch autograd dx: {x.grad}')\n",
    "print(f'Pytorch autograd dy: {y.grad}')\n",
    "\n",
    "dx = z.grad * y\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "dy = z.grad * x\n",
    "assert(torch.all(torch.isclose(dy, y.grad)))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-gabriel",
   "metadata": {},
   "source": [
    "### Vector product\n",
    "Forward pass:\n",
    "$$\n",
    "Z = xy\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{Z}}y^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{y}} = x^T\\frac{\\partial{L}}{\\partial{Z}}\n",
    "$$\n",
    "Where $[x]_{mx1}, [y]_{1xn}, [Z]_{mxn}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "informed-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[5.],\n",
      "        [5.],\n",
      "        [5.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[6., 6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64).reshape(3,1)\n",
    "y = torch.tensor([3, 2], requires_grad=True, dtype=torch.float64).reshape(1,2)\n",
    "z = x.matmul(y)\n",
    "x.retain_grad()\n",
    "y.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {y.grad}')\n",
    "\n",
    "dx = z.grad.matmul(y.T)\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "dy = x.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dy, y.grad)))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-convenience",
   "metadata": {},
   "source": [
    "### Vector & Matrix product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "Z = Wx\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{x}} = W^T\\frac{\\partial{L}}{\\partial{Z}} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{Z}}x^T\n",
    "$$\n",
    "Where $[x]_{Dx1}, [W]_{CxD}, [z]_{Cx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abroad-elder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1.],\n",
      "        [1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[3.],\n",
      "        [3.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[1., 2.],\n",
      "        [1., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2], requires_grad=True, dtype=torch.float64).reshape(2,1)\n",
    "w = torch.tensor([[1, 2],[2, 1]], requires_grad=True, dtype=torch.float64).reshape(2,2)\n",
    "z = w.matmul(x)\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {w.grad}')\n",
    "\n",
    "dx = w.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dx, x.grad)))\n",
    "\n",
    "dw = z.grad.matmul(x.T)\n",
    "assert(torch.allclose(dw, w.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-muscle",
   "metadata": {},
   "source": [
    "### Matrix & Matrix product\n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "Z = XW\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{X}} = \\frac{\\partial{L}}{\\partial{Z}}W^T \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}} = X^T\\frac{\\partial{L}}{\\partial{Z}}\n",
    "$$\n",
    "Where $[x]_{NxD}, [W]_{DxC}, [z]_{NxC}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "exposed-drinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dz:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "Pytorch autograd dx:\n",
      " tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], dtype=torch.float64)\n",
      "Pytorch autograd dy:\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([[1, 2, 3], [3, 2, 1]], requires_grad=True, dtype=torch.float64).reshape(2,3)\n",
    "w = torch.tensor([[1, 2, 3],[2, 1, 3], [3, 1, 2]], requires_grad=True, dtype=torch.float64).reshape(3,3)\n",
    "z = x.matmul(w)\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "z.retain_grad()\n",
    "L = z.sum()\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dz:\\n {z.grad}')\n",
    "print(f'Pytorch autograd dx:\\n {x.grad}')\n",
    "print(f'Pytorch autograd dy:\\n {w.grad}')\n",
    "\n",
    "dw = x.T.matmul(z.grad)\n",
    "assert(torch.all(torch.isclose(dw, w.grad)))\n",
    "\n",
    "dx = z.grad.matmul(w.T)\n",
    "assert(torch.allclose(dx, x.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-kingston",
   "metadata": {},
   "source": [
    "### Softmax \n",
    "\n",
    "Forward pass:\n",
    "$$\n",
    "p_i = \\frac{e^{s_i}}{\\sum_i{e^{s_i}}} \\\\\n",
    "t = \\sum_i{e^{s_i}}\n",
    "$$\n",
    "\n",
    "Backward pass:\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{s}} = J^p_s\\frac{\\partial{L}}{\\partial{p}} \\\\\n",
    "J^{p_i}_{s_j} = -p_ip_j \\ (i \\neq j)\\\\\n",
    "J^{p_i}_{s_i} = p_i(1-p_i)\n",
    "$$\n",
    "Where $[s]_{Cx1}, [p]_{Cx1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "scheduled-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch autograd dp:\n",
      " tensor([[-3.7183],\n",
      "        [-1.3679]], dtype=torch.float64)\n",
      "Pytorch autograd ds:\n",
      " tensor([[-0.4621],\n",
      "        [ 0.4621]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Vector product\n",
    "\n",
    "import torch\n",
    "s = torch.tensor([[1, 2]], requires_grad=True, dtype=torch.float64).reshape(2,1)\n",
    "s_exp = torch.exp(s)\n",
    "t = torch.sum(s_exp)\n",
    "p = s_exp / t\n",
    "s.retain_grad()\n",
    "p.retain_grad()\n",
    "# log likelihood\n",
    "L = -torch.sum(torch.log(p))\n",
    "L.backward()\n",
    "print(f'Pytorch autograd dp:\\n {p.grad}')\n",
    "print(f'Pytorch autograd ds:\\n {s.grad}')\n",
    "\n",
    "J = torch.zeros(2, 2, dtype=p.dtype)\n",
    "J[0, 0] = p[0]*(1-p[0]) # i = 0, i = 0\n",
    "J[1, 0] = -p[0]*p[1]    # i = 0, j = 1\n",
    "J[0, 1] = -p[1]*p[0]     # i = 1, j = 0\n",
    "J[1, 1] = p[1]*(1-p[1]) # i = 0, i = 1\n",
    "ds = J.matmul(p.grad)\n",
    "\n",
    "assert(torch.allclose(ds, s.grad))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-flush",
   "metadata": {},
   "source": [
    "## Computational Graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
