{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "skilled-costume",
   "metadata": {},
   "source": [
    "# Machine Learning Notes - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-standard",
   "metadata": {},
   "source": [
    "## A List of Related Posts\n",
    "1. [Pytorch]({% post_url 2021-05-04-machine-learning-pytorch %})\n",
    "2. [Loss function]({% post_url 2021-05-07-machine-learning-loss %})\n",
    "3. [Backpropagation(this)]({% post_url 2021-05-07-machine-learning-backpropagation %})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-warner",
   "metadata": {},
   "source": [
    "## Backpropagation Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-stable",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\n",
    "y = f_1(x) \\\\\n",
    "L = f_2(y)\n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = \\frac{\\partial{L}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{x}} = \\frac{\\partial{f_2(y)}}{\\partial{y}} \\frac{\\partial{f_1(x)}}{\\partial{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-indonesian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: 6\n",
      "Pytorch autograd tensor([6.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([5], requires_grad=True, dtype=torch.float64)\n",
    "y = 2 * x\n",
    "L = 3 * y\n",
    "dL = 1\n",
    "dy = 3 * dL\n",
    "dx = 2 * dy\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-mustang",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix} = f(\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "f_{1}(x_1, x_2, x_3) \\\\\n",
    "f_{2}(x_1, x_2, x_3)\n",
    "\\end{bmatrix}\\\\\n",
    "L = g(\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{bmatrix}) \\\\ \n",
    "$$\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{y}} = \\begin{bmatrix} \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_1}} \\\\ \n",
    "    \\frac{\\partial{g(y)}}{\\partial{y_2}} \\\\ \n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{x}} = J_x^y \\frac{\\partial{L}}{\\partial{y}} =\n",
    "\\begin{bmatrix} \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_1}}, \\frac{\\partial{f_2(x)}}{\\partial{x_1}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_2}}, \\frac{\\partial{f_2(x)}}{\\partial{x_2}}\\\\ \n",
    "    \\frac{\\partial{f_1(x)}}{\\partial{x_3}}, \\frac{\\partial{f_2(x)}}{\\partial{x_3}}\n",
    "\\end{bmatrix} \\frac{\\partial{L}}{\\partial{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: tensor([1, 3, 2])\n",
      "Pytorch autograd tensor([1., 3., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gradients with respect to a vector\n",
    "\n",
    "# Chain Rule\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float64)\n",
    "y = torch.zeros(2)\n",
    "y[0] = x[0] + 2*x[1]\n",
    "y[1] = x[1] + 2*x[2]\n",
    "L = y.sum()\n",
    "dL = 1\n",
    "dy = torch.tensor([1, 1])\n",
    "dx = torch.tensor([[1, 0],[2, 1], [0, 2]]).matmul(dy)\n",
    "L.backward()\n",
    "print(f'dL/dx: {dx}')\n",
    "print(f'Pytorch autograd {x.grad}')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-adolescent",
   "metadata": {},
   "source": [
    "## Local Gradient\n",
    "\n",
    "... ---> $\\textbf{x}$ ---> $f(\\textbf{x})$ ---> $\\textbf{y}$ ---> ... ---> L\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{L}}{\\partial{\\textbf{x}}} = J^{\\textbf{y}}_\\textbf{x}\\frac{\\partial{L}}{\\partial{\\textbf{y}}} \n",
    "$$\n",
    "$J^{\\textbf{y}}_\\textbf{x}$ is a Jacobian matrix, and it has the same number of rows as $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-chicken",
   "metadata": {},
   "source": [
    "## Common Computation Blocks - Forward and Backward Pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
