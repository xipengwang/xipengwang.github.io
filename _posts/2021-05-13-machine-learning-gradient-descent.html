---
layout: notebook
title: Machine Learning Notes - Gradient Descent
description: Machine Learning Notes - gradient descent, SGD, Momentum, RMSProp, Adam
tag: ML
---

<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Introduction">
    Introduction
    <a class="anchor-link" href="#Introduction">
     ¶
    </a>
   </h2>
   <p>
    <a href="{% post_url 2021-05-04-machine-learning-pytorch %}#pytorch-section-optimizer">
     Pythoch optimizer
    </a>
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Stochastic-Gradient-Descent-(SGD)">
    Stochastic Gradient Descent (SGD)
    <a class="anchor-link" href="#Stochastic-Gradient-Descent-(SGD)">
     ¶
    </a>
   </h2>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   $$w := w - \text{learning_rate} \times \frac{\partial{L}}{\partial{w}}$$
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Performs vanilla stochastic gradient descent.</span>
<span class="sd">    config format:</span>
<span class="sd">      - learning_rate: Scalar learning rate.</span>
<span class="sd">    '''</span>
    
    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">-=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'learning_rate'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dw</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">config</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="SGD-+-Momentum">
    SGD + Momentum
    <a class="anchor-link" href="#SGD-+-Momentum">
     ¶
    </a>
   </h2>
   $$
v = \mu \times v - \text{learning_rate} \times \frac{\partial{L}}{\partial{w}} \\
w := w + v
$$
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">def</span> <span class="nf">sgd_momentum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Performs stochastic gradient descent with momentum.</span>
<span class="sd">    config format:</span>
<span class="sd">      - learning_rate: Scalar learning rate.</span>
<span class="sd">      - momentum: Scalar between 0 and 1 giving the momentum value.</span>
<span class="sd">        Setting momentum = 0 reduces to sgd.</span>
<span class="sd">      - velocity: A numpy array of the same shape as w and dw used to store a</span>
<span class="sd">        moving average of the gradients.</span>
<span class="sd">    '''</span>
    
    <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'momentum'</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'velocity'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="n">v</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span><span class="o">*</span><span class="n">v</span><span class="o">-</span><span class="n">config</span><span class="p">[</span><span class="s1">'learning_rate'</span><span class="p">]</span><span class="o">*</span><span class="n">dw</span>
    <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">v</span>
    
    <span class="n">config</span><span class="p">[</span><span class="s1">'velocity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="RMSProp">
    RMSProp
    <a class="anchor-link" href="#RMSProp">
     ¶
    </a>
   </h2>
   <p>
    RMSProp is an update rule that set per-parameter learning rates by using a running average of the second moments of gradients.
   </p>
   $$
\text{cache} = \text{decay_rate} \times \text{cache} + (1 - \text{decay_rate}) \times \frac{\partial{L}}{\partial{w}} .* \frac{\partial{L}}{\partial{w}}\\
w := w - \frac{\text{learning_rate} \times \frac{\partial{L}}{\partial{w}}}{(\sqrt{\text{cache}} + \epsilon)}
$$
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">def</span> <span class="nf">rmsprop</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">'''</span>
<span class="sd">  Uses the RMSProp update rule, which uses a moving average of squared</span>
<span class="sd">  gradient values to set adaptive per-parameter learning rates.</span>
<span class="sd">  config format:</span>
<span class="sd">  - learning_rate: Scalar learning rate.</span>
<span class="sd">  - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span>
<span class="sd">    gradient cache.</span>
<span class="sd">  - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span>
<span class="sd">  - cache: Moving average of second moments of gradients.</span>
<span class="sd">  '''</span>

  <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'decay_rate'</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'epsilon'</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'cache'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>

  <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">config</span><span class="p">[</span><span class="s1">'cache'</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'decay_rate'</span><span class="p">]</span><span class="o">*</span><span class="n">config</span><span class="p">[</span><span class="s1">'cache'</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s1">'decay_rate'</span><span class="p">])</span> <span class="o">*</span> <span class="n">dw</span> <span class="o">*</span> <span class="n">dw</span>
  <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span><span class="o">-</span><span class="n">config</span><span class="p">[</span><span class="s1">'learning_rate'</span><span class="p">]</span><span class="o">*</span><span class="n">dw</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">'cache'</span><span class="p">])</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s1">'epsilon'</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Adam">
    Adam
    <a class="anchor-link" href="#Adam">
     ¶
    </a>
   </h2>
   <p>
    Adam extends RMSprop with a first-order gradient cache similar to momentum, and a bias correction mechanism to prevent large steps at the start of optimization. 
$$
m = \beta_1 \times m + (1-\beta_1) \times \frac{\partial{L}}{\partial{w}} \\
v = \beta_2 \times v + (1-\beta_2)\times(\frac{\partial{L}}{\partial{w}}.*\frac{\partial{L}}{\partial{w}}) \\
x += - \frac{\text{learning_rate} \times m}{\sqrt{v} + eps} \\
$$
   </p>
   <p>
    The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors $m$,$v$ are both initialized and therefore biased at zero, before they fully “warm up”. With the bias correction mechanism, the update looks as follows:
$$
m = \beta_1 \times m + (1-\beta_1)\times \frac{\partial{L}}{\partial{w}} \\
m_t = \frac{m}{1-\beta_1^t} \\ 
v = \beta_2 \times v + (1-\beta_2) \times (\frac{\partial{L}}{\partial{w}}.* \frac{\partial{L}}{\partial{w}}) \\
v_t = \frac{v}{1-\beta_2^t} \\
w := w - \frac{\text{learning_rate} \times m_t}{\sqrt{v_t} + \epsilon} \\ 
$$
    <strong>
     Where $t$ is your iteration counter going from $1$ to infinity
    </strong>
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [14]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">def</span> <span class="nf">adam</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">'''</span>
<span class="sd">  Uses the Adam update rule, which incorporates moving averages of both the</span>
<span class="sd">  gradient and its square and a bias correction term.</span>
<span class="sd">  config format:</span>
<span class="sd">  - learning_rate: Scalar learning rate.</span>
<span class="sd">  - beta1: Decay rate for moving average of first moment of gradient.</span>
<span class="sd">  - beta2: Decay rate for moving average of second moment of gradient.</span>
<span class="sd">  - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span>
<span class="sd">  - m: Moving average of gradient.</span>
<span class="sd">  - v: Moving average of squared gradient.</span>
<span class="sd">  - t: Iteration number.</span>
<span class="sd">  '''</span>

  <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'learning_rate'</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'beta1'</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'beta2'</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'epsilon'</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'m'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
  <span class="n">config</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">'t'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="n">next_w</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">config</span><span class="p">[</span><span class="s1">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'t'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="n">b1</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'beta1'</span><span class="p">]</span>
  <span class="n">b2</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'beta2'</span><span class="p">]</span>
  <span class="n">t</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">'t'</span><span class="p">]</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">b1</span><span class="o">*</span><span class="n">config</span><span class="p">[</span><span class="s1">'m'</span><span class="p">]</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="p">)</span><span class="o">*</span><span class="n">dw</span>
  <span class="n">mt</span> <span class="o">=</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b1</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">b2</span><span class="o">*</span><span class="n">config</span><span class="p">[</span><span class="s1">'v'</span><span class="p">]</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="p">)</span><span class="o">*</span><span class="n">dw</span><span class="o">*</span><span class="n">dw</span>
  <span class="n">vt</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">b2</span><span class="o">**</span><span class="n">t</span><span class="p">)</span>
  <span class="n">next_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">config</span><span class="p">[</span><span class="s1">'learning_rate'</span><span class="p">]</span><span class="o">*</span><span class="n">mt</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vt</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s1">'epsilon'</span><span class="p">])</span>
  <span class="n">config</span><span class="p">[</span><span class="s1">'m'</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span>
  <span class="n">config</span><span class="p">[</span><span class="s1">'v'</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
  <span class="k">return</span> <span class="n">next_w</span><span class="p">,</span> <span class="n">config</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>

<div> This blog is converted from <a href="https://github.com/xipengwang/xipengwang.github.io/tree/master/notebooks/machine-learning-gradient-descent.ipynb">machine-learning-gradient-descent.ipynb</a></div>